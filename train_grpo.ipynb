{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c673fb17-264c-442b-ab45-d6444a3f48da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.51s/it]\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training from checkpoint: cai6307-henrykobs/model/Qwen-7B-GRPO-2.0/checkpoint-240\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='307' max='934' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [307/934 1:47:22 < 17:15:49, 0.01 it/s, Epoch 0.33/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>241</td>\n",
       "      <td>0.179400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242</td>\n",
       "      <td>0.097500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>243</td>\n",
       "      <td>0.079300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>244</td>\n",
       "      <td>0.147800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>-0.023300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>0.028700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>0.152500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>0.096200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>-0.005100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.064700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>251</td>\n",
       "      <td>0.012500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>0.017200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>253</td>\n",
       "      <td>0.021600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>254</td>\n",
       "      <td>0.048200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>0.043300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>257</td>\n",
       "      <td>0.145500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>258</td>\n",
       "      <td>-0.005300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>259</td>\n",
       "      <td>0.062100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.270200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>261</td>\n",
       "      <td>-0.017500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>262</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>263</td>\n",
       "      <td>0.028100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>264</td>\n",
       "      <td>-0.039700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>0.069700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>266</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>267</td>\n",
       "      <td>0.105000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268</td>\n",
       "      <td>0.105600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>269</td>\n",
       "      <td>-0.044000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.016200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>271</td>\n",
       "      <td>0.039000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>272</td>\n",
       "      <td>0.165800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>273</td>\n",
       "      <td>0.020600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>274</td>\n",
       "      <td>-0.110500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>-0.102900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>276</td>\n",
       "      <td>0.032000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>277</td>\n",
       "      <td>0.001900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>278</td>\n",
       "      <td>-0.015900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>-0.028400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>-0.021000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>281</td>\n",
       "      <td>-0.008500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>282</td>\n",
       "      <td>-0.011700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>283</td>\n",
       "      <td>0.051400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284</td>\n",
       "      <td>-0.077300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>0.171200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>286</td>\n",
       "      <td>0.070700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>287</td>\n",
       "      <td>0.009000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>-0.177200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>289</td>\n",
       "      <td>-0.012400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.171500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>291</td>\n",
       "      <td>0.100100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>292</td>\n",
       "      <td>0.117200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>293</td>\n",
       "      <td>0.084800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>294</td>\n",
       "      <td>-0.078900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>0.020400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>296</td>\n",
       "      <td>0.058100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>297</td>\n",
       "      <td>0.100800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>298</td>\n",
       "      <td>0.039300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>299</td>\n",
       "      <td>-0.060400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>-0.079000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>301</td>\n",
       "      <td>-0.113200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>302</td>\n",
       "      <td>0.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>303</td>\n",
       "      <td>-0.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>0.043400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>0.017200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=2048) and `max_length`(=89) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=2048) and `max_length`(=81) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=2048) and `max_length`(=111) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=2048) and `max_length`(=104) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=2048) and `max_length`(=76) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated average response length: 499.00 tokens\n",
      "Saved loss plot at step 250\n",
      "Saved reward plot at step 250\n",
      "Saved response length plot at step 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=2048) and `max_length`(=89) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=2048) and `max_length`(=81) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=2048) and `max_length`(=111) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=2048) and `max_length`(=104) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=2048) and `max_length`(=76) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated average response length: 316.20 tokens\n",
      "Saved loss plot at step 260\n",
      "Saved reward plot at step 260\n",
      "Saved response length plot at step 260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=2048) and `max_length`(=89) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=2048) and `max_length`(=81) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=2048) and `max_length`(=111) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=2048) and `max_length`(=104) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=2048) and `max_length`(=76) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated average response length: 321.80 tokens\n",
      "Saved loss plot at step 270\n",
      "Saved reward plot at step 270\n",
      "Saved response length plot at step 270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=2048) and `max_length`(=89) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=2048) and `max_length`(=81) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=2048) and `max_length`(=111) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=2048) and `max_length`(=104) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=2048) and `max_length`(=76) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated average response length: 126.20 tokens\n",
      "Saved loss plot at step 280\n",
      "Saved reward plot at step 280\n",
      "Saved response length plot at step 280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=2048) and `max_length`(=89) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=2048) and `max_length`(=81) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=2048) and `max_length`(=111) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=2048) and `max_length`(=104) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=2048) and `max_length`(=76) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated average response length: 258.60 tokens\n",
      "Saved loss plot at step 290\n",
      "Saved reward plot at step 290\n",
      "Saved response length plot at step 290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=2048) and `max_length`(=89) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=2048) and `max_length`(=81) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=2048) and `max_length`(=111) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=2048) and `max_length`(=104) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=2048) and `max_length`(=76) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated average response length: 277.80 tokens\n",
      "Saved loss plot at step 300\n",
      "Saved reward plot at step 300\n",
      "Saved response length plot at step 300\n"
     ]
    }
   ],
   "source": [
    "# train_grpo_improved.py\n",
    "#\n",
    "# See https://github.com/willccbb/verifiers for ongoing developments\n",
    "\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainerCallback\n",
    "from peft import LoraConfig\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "# Define cache directory\n",
    "CACHE_DIR = \"cai6307-henrykobs/cache\"\n",
    "\n",
    "# Load and prep dataset\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format, you have to adhere to the format, only output the final answer without **ANY** additional information in the \"answer\" box.\n",
    "\n",
    "<think>\n",
    "...\n",
    "</think>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "XML_COT_FORMAT = \"\"\"\\\n",
    "<think>\n",
    "{think}\n",
    "</think>\n",
    "<answer>\n",
    "{answer}\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "def extract_xml_answer(text: str):\n",
    "    answer = text.split(\"<answer>\")[-1]\n",
    "    answer = answer.split(\"</answer>\")[0]\n",
    "    return answer.strip()\n",
    "\n",
    "def extract_hash_answer(text: str):\n",
    "    if \"####\" not in text:\n",
    "        return None\n",
    "    return text.split(\"####\")[1].strip().replace(\",\", \"\").replace(\"$\", \"\")\n",
    "\n",
    "def get_gsm8k_questions(split=\"train\"):\n",
    "    data = load_dataset('openai/gsm8k', 'main', cache_dir=CACHE_DIR)[split]  # specify cache directory\n",
    "    data = data.map(lambda x: {\n",
    "        'prompt': [\n",
    "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "            {'role': 'user', 'content': x['question']}\n",
    "        ],\n",
    "        'answer': extract_hash_answer(x['answer'])\n",
    "    })\n",
    "    return data\n",
    "\n",
    "dataset = get_gsm8k_questions()\n",
    "\n",
    "# Reward functions\n",
    "def correctness_reward_func(prompts, completions, answer, **kwargs):\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    q = prompts[0][-1]['content']\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n",
    "\n",
    "def int_reward_func(completions, **kwargs):\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]\n",
    "\n",
    "def strict_format_reward_func(completions, **kwargs):\n",
    "    pattern = r\"^<think>\\n.*?\\n</think>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r, flags=re.DOTALL) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def soft_format_reward_func(completions, **kwargs):\n",
    "    pattern = r\"<think>.*?</think>\\s*<answer>.*?</answer>\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r, flags=re.DOTALL) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def count_xml(text) -> float:\n",
    "    count = 0.0\n",
    "    if text.count(\"<think>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"\\n</think>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"\\n<answer>\\n\") == 1:\n",
    "        count += 0.125\n",
    "        count -= len(text.split(\"\\n</answer>\\n\")[-1]) * 0.001\n",
    "    if text.count(\"\\n</answer>\") == 1:\n",
    "        count += 0.125\n",
    "        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1) * 0.001\n",
    "    return count\n",
    "\n",
    "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
    "    contents = [completion[0][\"content\"] for completion in completions]\n",
    "    return [count_xml(c) for c in contents]\n",
    "\n",
    "# Choose model and set up output parameters\n",
    "# model_name = \"Qwen/Qwen2.5-7B-Instruct-1M\"\n",
    "# model_name = \"Qwen/Qwen2.5-14B-Instruct-1M\"\n",
    "model_name = \"Qwen/Qwen2-7B\"\n",
    "\n",
    "if \"Llama\" in model_name:\n",
    "    output_dir = \"cai6307-henrykobs/model/Llama-1B-GRPO\"\n",
    "    run_name = \"Llama-1B-GRPO-gsm8k\"\n",
    "else:\n",
    "    output_dir = \"cai6307-henrykobs/model/Qwen-7B-GRPO-2.0\"\n",
    "    run_name = \"Qwen-7B-GRPO-gsm8k-2.0\"\n",
    "\n",
    "# Training configuration optimized for an A100 80GB\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=output_dir,\n",
    "    run_name=run_name,\n",
    "    learning_rate=1e-5,  # Slightly increased learning rate (tune as needed)\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.99,\n",
    "    weight_decay=0.1,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type='cosine',\n",
    "    logging_steps=1,\n",
    "    bf16=True,\n",
    "    per_device_train_batch_size=4,      # Increased batch size for ample GPU memory\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_generations=2,                   # Single generation per prompt\n",
    "    max_prompt_length=256,\n",
    "    max_completion_length=512,\n",
    "    num_train_epochs=1,\n",
    "    save_steps=20,                      # Save checkpoint every 100 steps\n",
    "    max_grad_norm=0.1,\n",
    "    report_to=\"none\",                    # Change to \"tensorboard\" if you want native TensorBoard logging\n",
    "    log_on_each_node=False,\n",
    "    scale_rewards=False\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"up_proj\", \"down_proj\", \"gate_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    lora_dropout=0.05,\n",
    ")\n",
    "\n",
    "# Initialize the model loaded entirely on GPU (remove offloading)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    cache_dir=CACHE_DIR,  # specify cache directory\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Optionally, if you are using PyTorch 2.0+, you can compile the model for a speed boost:\n",
    "# model = torch.compile(model)\n",
    "\n",
    "# Gradient checkpointing is not required with 80GB, so it's been disabled for potentially faster execution\n",
    "# model.gradient_checkpointing_enable()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=CACHE_DIR)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# --- Custom Callback for Plotting Metrics Separately ---\n",
    "class PlottingCallback(TrainerCallback):\n",
    "    def __init__(self, model, tokenizer, sample_prompts, eval_steps=10, eval_max_length=50):\n",
    "        \"\"\"\n",
    "        :param model: The current model.\n",
    "        :param tokenizer: The tokenizer for processing prompts.\n",
    "        :param sample_prompts: A list of sample prompts to evaluate response lengths.\n",
    "                               Each prompt should be a list of messages (dict with \"content\").\n",
    "        :param eval_steps: Frequency (in global steps) to evaluate and log response length.\n",
    "        :param eval_max_length: Maximum additional tokens to generate for evaluation.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sample_prompts = sample_prompts\n",
    "        self.eval_steps = eval_steps\n",
    "        self.eval_max_length = eval_max_length\n",
    "\n",
    "        self.global_steps = []\n",
    "        self.losses = []\n",
    "        self.rewards = []\n",
    "        self.avg_resp_lengths = []\n",
    "        self.resp_plot_steps = []\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is None:\n",
    "            return\n",
    "\n",
    "        # Collect metrics if present.\n",
    "        if \"loss\" in logs:\n",
    "            self.global_steps.append(state.global_step)\n",
    "            self.losses.append(logs[\"loss\"])\n",
    "        if \"reward\" in logs:\n",
    "            self.rewards.append(logs[\"reward\"])\n",
    "\n",
    "        # Every eval_steps, compute average response length.\n",
    "        if state.global_step % self.eval_steps == 0:\n",
    "            avg_length = self.evaluate_response_length()\n",
    "            self.avg_resp_lengths.append(avg_length)\n",
    "            self.resp_plot_steps.append(state.global_step)\n",
    "            self.plot_all_metrics()\n",
    "\n",
    "    def evaluate_response_length(self):\n",
    "        lengths = []\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for prompt in self.sample_prompts:\n",
    "                # Use the last message's content as the prompt text.\n",
    "                prompt_text = prompt[-1][\"content\"]\n",
    "                encoded = self.tokenizer(prompt_text, return_tensors=\"pt\").to(self.model.device)\n",
    "                input_length = encoded.input_ids.shape[1]\n",
    "                outputs = self.model.generate(\n",
    "                    **encoded,\n",
    "                    max_length=input_length + self.eval_max_length,\n",
    "                    do_sample=True\n",
    "                )\n",
    "                # Compute number of generated tokens (excluding the prompt)\n",
    "                gen_length = outputs.shape[1] - input_length\n",
    "                lengths.append(gen_length)\n",
    "        self.model.train()\n",
    "        avg_length = sum(lengths) / len(lengths) if lengths else 0\n",
    "        print(f\"Evaluated average response length: {avg_length:.2f} tokens\")\n",
    "        return avg_length\n",
    "\n",
    "    def plot_all_metrics(self):\n",
    "        # Plot Loss\n",
    "        plt.figure()\n",
    "        plt.plot(self.global_steps, self.losses, label=\"Loss\")\n",
    "        plt.xlabel(\"Global Steps\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Loss over Time\")\n",
    "        plt.legend()\n",
    "        plt.savefig(\"run3/loss_metrics.png\")\n",
    "        plt.close()\n",
    "        print(f\"Saved loss plot at step {self.global_steps[-1]}\")\n",
    "\n",
    "        # Plot Reward (if available)\n",
    "        if self.rewards:\n",
    "            plt.figure()\n",
    "            # Make sure we plot rewards vs. steps corresponding to rewards.\n",
    "            plt.plot(self.global_steps[:len(self.rewards)], self.rewards, label=\"Reward\")\n",
    "            plt.xlabel(\"Global Steps\")\n",
    "            plt.ylabel(\"Reward\")\n",
    "            plt.title(\"Reward over Time\")\n",
    "            plt.legend()\n",
    "            plt.savefig(\"run3/reward_metrics.png\")\n",
    "            plt.close()\n",
    "            print(f\"Saved reward plot at step {self.global_steps[-1]}\")\n",
    "\n",
    "        # Plot Average Response Length\n",
    "        plt.figure()\n",
    "        plt.plot(self.resp_plot_steps, self.avg_resp_lengths, label=\"Avg Response Length (tokens)\")\n",
    "        plt.xlabel(\"Global Steps\")\n",
    "        plt.ylabel(\"Tokens\")\n",
    "        plt.title(\"Average Response Length over Time\")\n",
    "        plt.legend()\n",
    "        plt.savefig(\"run3/response_length.png\")\n",
    "        plt.close()\n",
    "        print(f\"Saved response length plot at step {self.global_steps[-1]}\")\n",
    "\n",
    "# Select a few sample prompts from the dataset for evaluation.\n",
    "# Here we take the first 5 samples and use their \"prompt\" field.\n",
    "sample_prompts = [item[\"prompt\"] for item in dataset.select(range(5))]\n",
    "\n",
    "# Initialize the GRPOTrainer with the custom callback.\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    reward_funcs=[\n",
    "        correctness_reward_func,\n",
    "        soft_format_reward_func,\n",
    "        int_reward_func\n",
    "    ],\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "trainer.add_callback(PlottingCallback(model, tokenizer, sample_prompts, eval_steps=10, eval_max_length=50))\n",
    "\n",
    "# --- Checkpoint Recovery ---\n",
    "# If the training was interrupted, resume from the latest checkpoint if available.\n",
    "resume_checkpoint = None\n",
    "if os.path.exists(output_dir):\n",
    "    checkpoints = [d for d in os.listdir(output_dir) if d.startswith(\"checkpoint\")]\n",
    "    if checkpoints:\n",
    "        # Sort checkpoints by the numeric step value appended at the end of the folder name.\n",
    "        checkpoints = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[-1]))\n",
    "        resume_checkpoint = os.path.join(output_dir, checkpoints[-1])\n",
    "        print(f\"Resuming training from checkpoint: {resume_checkpoint}\")\n",
    "\n",
    "# --- Start Training ---\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        trainer.train(resume_from_checkpoint=resume_checkpoint)\n",
    "    except Exception as e:\n",
    "        # If something goes wrong, save the current state so we can resume later.\n",
    "        print(\"An error occurred during training:\", e)\n",
    "        trainer.save_state()\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a314fdc0-1039-47b0-876b-7e0eb8222077",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "CACHE_DIR = \"cai6307-henrykobs/cache\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct\", cache_DIR=CACHE_DIR)\n",
    "print(\"chegou\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct\",cache_dir=CACHE_DIR, device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9173bf5-88cd-4f07-aad3-cb150769d019",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Flash Attention 2 not found. Install with `pip install flash-attn --no-build-isolation` for potential speedup.\n",
      "Applying template: 100%|██████████| 263/263 [00:00<00:00, 6898.22it/s]\n",
      "Loading checkpoint shards:  75%|███████▌  | 3/4 [00:15<00:05,  5.01s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 548\u001b[0m\n\u001b[1;32m    545\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax New Tokens     : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mGENERATION_MAX_NEW_TOKENS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    546\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 548\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 399\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    397\u001b[0m base_dataset_prepared \u001b[38;5;241m=\u001b[39m prepare_prompts(dataset, base_tokenizer)\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(base_dataset_prepared) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 399\u001b[0m      base_acc \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m         \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_dataset_prepared\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbase_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdebug_sample_output\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Pass debug flag\u001b[39;49;00m\n\u001b[1;32m    402\u001b[0m \u001b[43m     \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m      results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase_model\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m base_acc\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 172\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model_id, dataset, batch_size, tokenizer, debug_sample_output)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m# Load pipeline\u001b[39;00m\n\u001b[0;32m--> 172\u001b[0m     pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Use pre-loaded tokenizer\u001b[39;49;00m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTORCH_DTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattn_implementation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mATTN_IMPLEMENTATION\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mATTN_IMPLEMENTATION\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mflash_attention_2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Only pass if using flash\u001b[39;49;00m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;66;03m# Ensure padding token setup for batching\u001b[39;00m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pipe\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m pipe\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/pipelines/__init__.py:942\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    941\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m--> 942\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    952\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    953\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/pipelines/base.py:291\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel might be a PyTorch model (ending with `.bin`) but PyTorch is not available. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to load the model with Tensorflow.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    288\u001b[0m     )\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 291\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    293\u001b[0m         model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:573\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    572\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 573\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    579\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/modeling_utils.py:272\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/modeling_utils.py:4480\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4470\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4471\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   4473\u001b[0m     (\n\u001b[1;32m   4474\u001b[0m         model,\n\u001b[1;32m   4475\u001b[0m         missing_keys,\n\u001b[1;32m   4476\u001b[0m         unexpected_keys,\n\u001b[1;32m   4477\u001b[0m         mismatched_keys,\n\u001b[1;32m   4478\u001b[0m         offload_index,\n\u001b[1;32m   4479\u001b[0m         error_msgs,\n\u001b[0;32m-> 4480\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4485\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4486\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4490\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4497\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4500\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   4501\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/modeling_utils.py:4908\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, low_cpu_mem_usage, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only, _fast_init)\u001b[0m\n\u001b[1;32m   4905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m low_cpu_mem_usage:\n\u001b[1;32m   4906\u001b[0m     \u001b[38;5;66;03m# Skip it with fsdp on ranks other than 0\u001b[39;00m\n\u001b[1;32m   4907\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_fsdp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quantized):\n\u001b[0;32m-> 4908\u001b[0m         disk_offload_index, cpu_offload_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4909\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4910\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4911\u001b[0m \u001b[43m            \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4912\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4913\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreverse_key_renaming_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4914\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4915\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4916\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4917\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcpu_offload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcpu_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_offloaded_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m            \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4924\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4925\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4926\u001b[0m     assign_params \u001b[38;5;241m=\u001b[39m check_support_param_buffer_assignment(model_to_load, state_dict)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/modeling_utils.py:814\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, shard_file, expected_keys, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, cpu_offload_folder, cpu_offload_index, hf_quantizer, is_safetensors, keep_in_fp32_regex, unexpected_keys, device_mesh)\u001b[0m\n\u001b[1;32m    803\u001b[0m     shard_and_distribute_module(\n\u001b[1;32m    804\u001b[0m         model,\n\u001b[1;32m    805\u001b[0m         param,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    811\u001b[0m         device_mesh,\n\u001b[1;32m    812\u001b[0m     )\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 814\u001b[0m     param \u001b[38;5;241m=\u001b[39m \u001b[43mparam\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m casting_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    816\u001b[0m         param \u001b[38;5;241m=\u001b[39m param\u001b[38;5;241m.\u001b[39mto(casting_dtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import torch\n",
    "import logging\n",
    "import time\n",
    "import xml.etree.ElementTree as ET\n",
    "from types import SimpleNamespace\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- Flash Attention Check ---\n",
    "try:\n",
    "    import flash_attn\n",
    "    _flash_attn_available = True\n",
    "    logging.info(\"Flash Attention 2 available. Will use if applicable.\")\n",
    "except ImportError:\n",
    "    _flash_attn_available = False\n",
    "    logging.warning(\"Flash Attention 2 not found. Install with `pip install flash-attn --no-build-isolation` for potential speedup.\")\n",
    "\n",
    "# --- Constants ---\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format, you have to adhere to the format, only output the final answer without **ANY** additional information in the \"answer\" box.\n",
    "\n",
    "<think>\n",
    "...\n",
    "</think>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "DEFAULT_CACHE_DIR = \"cai6307-henrykobs/cache\"\n",
    "DEFAULT_BASE_MODEL = \"Qwen/Qwen2.5-7B-Instruct-1M\"\n",
    "DEFAULT_OUTPUT_DIR = \"cai6307-henrykobs/model/Qwen-7B-GRPO-2nd\"\n",
    "DEFAULT_EVAL_FRACTION = 0.2\n",
    "DEFAULT_BATCH_SIZE = 32 # Reduced default for demonstration if needed, adjust per GPU\n",
    "SEED = 42\n",
    "TORCH_DTYPE = torch.bfloat16\n",
    "ATTN_IMPLEMENTATION = \"flash_attention_2\" if _flash_attn_available else \"eager\"\n",
    "DEFAULT_DEBUG_SAMPLE_OUTPUT = True # Enable debug logging by default\n",
    "MAX_DEBUG_SAMPLES = 5 # Max *initial* raw samples to log\n",
    "GENERATION_MAX_NEW_TOKENS = 1024\n",
    "MAX_MISMATCH_LOG = 5\n",
    "\n",
    "# --- Logging Setup ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Extraction Functions ---\n",
    "def extract_xml_answer(text: str):\n",
    "    \"\"\"Extracts content within the first <answer>...</answer> tag, with fallbacks.\"\"\"\n",
    "    try:\n",
    "        start_tag = \"<answer>\"\n",
    "        end_tag = \"</answer>\"\n",
    "        start_idx = text.find(start_tag)\n",
    "        end_idx = text.find(end_tag, start_idx + len(start_tag)) # Search after start tag\n",
    "\n",
    "        if start_idx != -1 and end_idx != -1:\n",
    "             return text[start_idx + len(start_tag):end_idx].strip().lower()\n",
    "\n",
    "        # Fallback 1: Robust XML parsing\n",
    "        try:\n",
    "            xml_content = f\"<root>{text}</root>\" # Dummy root\n",
    "            root = ET.fromstring(xml_content)\n",
    "            answer_element = root.find('.//answer')\n",
    "            if answer_element is not None and answer_element.text is not None:\n",
    "                return answer_element.text.strip().lower()\n",
    "        except ET.ParseError:\n",
    "             pass # Malformed XML\n",
    "\n",
    "        # Fallback 2: Simple split\n",
    "        if start_tag in text and end_tag in text:\n",
    "             try:\n",
    "                 potential_answer = text.split(start_tag, 1)[1].split(end_tag, 1)[0]\n",
    "                 return potential_answer.strip().lower()\n",
    "             except IndexError:\n",
    "                 pass # Split failed\n",
    "\n",
    "        return None\n",
    "\n",
    "    except Exception as e:\n",
    "         logger.error(f\"Unexpected error during XML extraction: {e} for text: {text[:150]}...\", exc_info=True)\n",
    "         return None\n",
    "\n",
    "def extract_hash_answer(text: str):\n",
    "    \"\"\"Extracts the answer from GSM8K format '... #### <answer>'.\"\"\"\n",
    "    if \"####\" not in text:\n",
    "        return None\n",
    "    try:\n",
    "        answer = text.split(\"####\")[1].strip().replace(\",\", \"\").replace(\"$\", \"\").lower()\n",
    "        return answer\n",
    "    except IndexError:\n",
    "         logger.warning(f\"Could not split gold answer on '####': {text}\")\n",
    "         return None\n",
    "\n",
    "# --- Dataset Preparation ---\n",
    "def get_gsm8k_dataset(cache_dir: str, split=\"test\"):\n",
    "    \"\"\"Loads GSM8K, extracts gold answers, and filters invalid samples.\"\"\"\n",
    "    logger.info(f\"Loading GSM8K dataset ({split} split)...\")\n",
    "    try:\n",
    "        dataset = load_dataset(\"openai/gsm8k\", \"main\", cache_dir=cache_dir, split=split, trust_remote_code=True)\n",
    "        dataset = dataset.map(\n",
    "            lambda x: {\"gold_answer\": extract_hash_answer(x[\"answer\"]), \"question\": x[\"question\"]},\n",
    "            remove_columns=[\"answer\"],\n",
    "            desc=\"Extracting gold answers\"\n",
    "        )\n",
    "        initial_count = len(dataset)\n",
    "        dataset = dataset.filter(lambda x: x[\"gold_answer\"] is not None)\n",
    "        filtered_count = len(dataset)\n",
    "        if initial_count != filtered_count:\n",
    "             logger.warning(f\"Filtered {initial_count - filtered_count} samples with invalid gold answers.\")\n",
    "        logger.info(f\"Dataset loaded with {filtered_count} samples.\")\n",
    "        return dataset\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load or process dataset: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "def prepare_prompts(dataset: Dataset, tokenizer):\n",
    "    \"\"\"Applies the chat template to each question.\"\"\"\n",
    "    logger.info(f\"Applying chat template using {tokenizer.name_or_path}...\")\n",
    "    prompts = []\n",
    "    skipped_count = 0\n",
    "    for sample in tqdm(dataset, desc=\"Applying template\"):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": sample[\"question\"]},\n",
    "        ]\n",
    "        try:\n",
    "            # Important: Ensure the tokenizer is compatible with apply_chat_template\n",
    "            prompt_text = tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            prompts.append(prompt_text)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed template application for question (truncated): {sample['question'][:50]}... Error: {e}. Skipping.\")\n",
    "            prompts.append(None) # Placeholder for filtering\n",
    "            skipped_count += 1\n",
    "\n",
    "    # Add prompts and filter failures\n",
    "    dataset = dataset.add_column(\"prompt_text\", prompts)\n",
    "    original_len = len(prompts) # Count before filtering Nones\n",
    "    dataset = dataset.filter(lambda x: x[\"prompt_text\"] is not None)\n",
    "    if skipped_count > 0 or len(dataset) != original_len: # Check both ways\n",
    "        actual_filtered = original_len - len(dataset)\n",
    "        logger.warning(f\"Filtered out {actual_filtered} samples due to chat template errors.\")\n",
    "\n",
    "    if not dataset:\n",
    "         logger.error(\"Dataset is empty after applying chat template. Cannot proceed.\")\n",
    "         # Consider raising an error or handling this state appropriately upstream\n",
    "         return dataset # Return empty dataset\n",
    "\n",
    "    logger.info(f\"Chat template applied. {len(dataset)} prompts ready.\")\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# --- Evaluation Function ---\n",
    "@torch.no_grad()\n",
    "def evaluate_model(model_id: str, dataset: Dataset, batch_size: int, tokenizer, debug_sample_output: bool):\n",
    "    \"\"\"Evaluates a model using a pipeline, calculates accuracy, logs samples per batch if debug enabled.\"\"\"\n",
    "    logger.info(f\"Starting evaluation for: {model_id}\")\n",
    "    logger.info(f\"Using dtype={TORCH_DTYPE}, attn='{ATTN_IMPLEMENTATION}', max_new_tokens={GENERATION_MAX_NEW_TOKENS}\")\n",
    "    start_load_time = time.time()\n",
    "    pipe = None\n",
    "\n",
    "    # Check if dataset is empty before proceeding\n",
    "    if not dataset or len(dataset) == 0:\n",
    "        logger.error(f\"Dataset provided to evaluate_model for {model_id} is empty. Skipping evaluation.\")\n",
    "        return 0.0\n",
    "\n",
    "    try:\n",
    "        # Load pipeline\n",
    "        pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model_id,\n",
    "            tokenizer=tokenizer, # Use pre-loaded tokenizer\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=TORCH_DTYPE,\n",
    "            trust_remote_code=True,\n",
    "            model_kwargs={\"attn_implementation\": ATTN_IMPLEMENTATION} if ATTN_IMPLEMENTATION == \"flash_attention_2\" else {} # Only pass if using flash\n",
    "        )\n",
    "\n",
    "        # Ensure padding token setup for batching\n",
    "        if pipe.tokenizer.pad_token is None or pipe.tokenizer.pad_token_id is None:\n",
    "            logger.warning(f\"Tokenizer for {model_id} lacks pad token/ID. Setting pad_token = eos_token.\")\n",
    "            pipe.tokenizer.pad_token = pipe.tokenizer.eos_token\n",
    "            pipe.tokenizer.pad_token_id = pipe.tokenizer.eos_token_id # Make sure ID is set\n",
    "        if pipe.model.config.pad_token_id is None:\n",
    "             logger.warning(f\"Model config for {model_id} lacks pad_token_id. Setting from tokenizer: {pipe.tokenizer.pad_token_id}\")\n",
    "             pipe.model.config.pad_token_id = pipe.tokenizer.pad_token_id\n",
    "\n",
    "        # Use left-padding for decoder-only models during generation\n",
    "        pipe.tokenizer.padding_side = \"left\"\n",
    "        logger.info(f\"Tokenizer padding side: '{pipe.tokenizer.padding_side}', Pad token: '{pipe.tokenizer.pad_token}', ID: {pipe.tokenizer.pad_token_id}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"FATAL: Failed to load pipeline for {model_id}: {e}\", exc_info=True)\n",
    "        if pipe is not None: del pipe\n",
    "        torch.cuda.empty_cache()\n",
    "        return 0.0 # Indicate failure\n",
    "\n",
    "    load_time = time.time() - start_load_time\n",
    "    logger.info(f\"Pipeline loaded in {load_time:.2f}s.\")\n",
    "\n",
    "    logger.info(f\"Generating predictions for {len(dataset)} samples (batch size: {batch_size})...\")\n",
    "    predictions = []\n",
    "    generation_start_time = time.time()\n",
    "    debug_samples_logged = 0 # Counter for the *initial* debug samples\n",
    "\n",
    "    # Process dataset in batches using the pipeline\n",
    "    # The pipeline handles batching internally based on `batch_size`\n",
    "    # The loop iterates through results sample by sample, `i` is the dataset index\n",
    "    for i, output in enumerate(tqdm(pipe(dataset[\"prompt_text\"],\n",
    "                                           max_new_tokens=GENERATION_MAX_NEW_TOKENS,\n",
    "                                           do_sample=False, # Greedy decoding for consistency\n",
    "                                           batch_size=batch_size,\n",
    "                                           return_full_text=False, # Only generated part\n",
    "                                           pad_token_id=pipe.tokenizer.pad_token_id), # Crucial for batching\n",
    "                                      total=len(dataset), desc=f\"Generating {os.path.basename(model_id)}\")):\n",
    "        pred = None # Default prediction if extraction fails\n",
    "        generated_text = \"Error: Could not get generated text.\" # Default message\n",
    "\n",
    "        try:\n",
    "            # Standard output structure: [{'generated_text': '...'}]\n",
    "            generated_text = output[0]['generated_text']\n",
    "            pred = extract_xml_answer(generated_text) # Extract final answer\n",
    "\n",
    "            # --- Combined Debug Logging ---\n",
    "            if debug_sample_output:\n",
    "                # 1. Log first N raw outputs (original behavior)\n",
    "                if debug_samples_logged < MAX_DEBUG_SAMPLES:\n",
    "                    generated_length = len(tokenizer.encode(generated_text))\n",
    "                    logger.info(f\"\\n[DEBUG RAW OUTPUT {debug_samples_logged+1}/{MAX_DEBUG_SAMPLES} - {model_id}]\"\n",
    "                                f\"\\nRaw Output (Tokens: ~{generated_length}/{GENERATION_MAX_NEW_TOKENS}):\\n---\\n{generated_text}\\n---\"\n",
    "                                f\"\\nExtracted: {pred}\")\n",
    "                    if generated_length >= GENERATION_MAX_NEW_TOKENS - 5: # Small buffer\n",
    "                         logger.warning(f\"[DEBUG] Raw output length close to max_new_tokens. May be truncated.\")\n",
    "                    debug_samples_logged += 1\n",
    "\n",
    "                # 2. Log Question/Full Output/Gold for first sample of each batch\n",
    "                # Check if 'i' is the start of a batch (index 0, batch_size, 2*batch_size, etc.)\n",
    "                if i % batch_size == 0:\n",
    "                    try:\n",
    "                        question = dataset[i][\"question\"]\n",
    "                        gold_answer = dataset[i][\"gold_answer\"]\n",
    "                        batch_num = i // batch_size + 1\n",
    "                        total_batches = (len(dataset) + batch_size - 1) // batch_size\n",
    "                        logger.info(f\"\\n--- Sample from Batch {batch_num}/{total_batches} (Dataset Index {i}) ---\"\n",
    "                                    f\"\\nModel: {model_id}\"\n",
    "                                    f\"\\nQuestion: {question}\"\n",
    "                                    f\"\\nFull Model Output:\\n{generated_text}\"\n",
    "                                    f\"\\nGold Answer: {gold_answer}\"\n",
    "                                    f\"\\nExtracted Answer: {pred}\\n\"\n",
    "                                    f\"------------------------------------\")\n",
    "                    except IndexError:\n",
    "                         logger.warning(f\"Could not retrieve data for batch sample log at index {i}.\")\n",
    "                    except Exception as log_e:\n",
    "                         logger.warning(f\"Error during batch sample logging at index {i}: {log_e}\")\n",
    "            # --- End Combined Debug Logging ---\n",
    "\n",
    "        except (IndexError, KeyError, TypeError) as e:\n",
    "            # Handle pipeline output parsing issues\n",
    "            logger.warning(f\"Pipeline output parsing error at index {i}: {e}. Raw output object: {output}. Prediction set to None.\")\n",
    "            # Log the raw text if possible, even on error\n",
    "            raw_text_on_error = \"N/A\"\n",
    "            try: raw_text_on_error = output[0]['generated_text']\n",
    "            except Exception: pass\n",
    "            if debug_sample_output: logger.warning(f\"[DEBUG RAW ON PARSE ERROR]\\n---\\n{raw_text_on_error}\\n---\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # Catch any other unexpected errors during processing a single sample\n",
    "            logger.error(f\"Unexpected error processing sample {i} for {model_id}: {e}\", exc_info=True)\n",
    "            # generated_text is already set to an error message\n",
    "        finally:\n",
    "            # Always append a prediction (None if extraction failed or error occurred)\n",
    "            predictions.append(pred)\n",
    "\n",
    "    generation_time = time.time() - generation_start_time\n",
    "    samples_per_sec = len(dataset) / generation_time if generation_time > 0 else 0\n",
    "    logger.info(f\"Generation done in {generation_time:.2f}s ({samples_per_sec:.2f} samples/sec).\")\n",
    "\n",
    "    # Calculate Accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    mismatched_samples = []\n",
    "    logger.info(\"Calculating accuracy...\")\n",
    "    for i, (pred, gold) in enumerate(zip(predictions, dataset[\"gold_answer\"])):\n",
    "        if gold is None: continue # Should be filtered, but check\n",
    "        total += 1\n",
    "        if pred is not None and pred == gold:\n",
    "            correct += 1\n",
    "        elif len(mismatched_samples) < MAX_MISMATCH_LOG:\n",
    "             # Log mismatch details (question truncated)\n",
    "             mismatched_samples.append({\n",
    "                 \"index\": i, \"question\": dataset[i][\"question\"][:100]+\"...\",\n",
    "                 \"prediction\": pred, \"gold\": gold\n",
    "             })\n",
    "\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0.0\n",
    "    # Ensure total isn't zero before logging division\n",
    "    correct_str = f\"{correct}/{total}\" if total > 0 else \"0/0\"\n",
    "    logger.info(f\"Model {model_id} -- Accuracy: {accuracy:.2f}% ({correct_str})\")\n",
    "\n",
    "\n",
    "    if mismatched_samples:\n",
    "        logger.warning(f\"--- First {len(mismatched_samples)} Mismatched/Failed Samples ({model_id}) ---\")\n",
    "        for sample in mismatched_samples:\n",
    "            logger.warning(f\"Idx: {sample['index']}, Q: '{sample['question']}', Pred: '{sample['prediction']}', Gold: '{sample['gold']}'\")\n",
    "        logger.warning(\"--- End Mismatched Samples ---\")\n",
    "\n",
    "    # Cleanup\n",
    "    logger.info(f\"Cleaning up resources for {model_id}...\")\n",
    "    del pipe\n",
    "    # Explicitly clear cache if needed, might help prevent OOM in loops\n",
    "    torch.cuda.empty_cache()\n",
    "    logger.info(f\"Finished cleanup for {model_id}.\")\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# --- Tokenizer Cache Helper ---\n",
    "tokenizer_cache = {}\n",
    "def get_tokenizer(model_id_or_path: str, cache_dir: str):\n",
    "    \"\"\"Loads and caches tokenizer, ensuring pad token exists.\"\"\"\n",
    "    # Normalize path for consistent caching key\n",
    "    normalized_path = os.path.abspath(model_id_or_path) if os.path.exists(model_id_or_path) else model_id_or_path\n",
    "\n",
    "    if normalized_path in tokenizer_cache:\n",
    "        logger.info(f\"Using cached tokenizer for {model_id_or_path}\")\n",
    "        return tokenizer_cache[normalized_path]\n",
    "    else:\n",
    "        logger.info(f\"Loading tokenizer for {model_id_or_path}...\")\n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_id_or_path,\n",
    "                trust_remote_code=True,\n",
    "                cache_dir=cache_dir,\n",
    "                padding_side='left' # Set padding side during load\n",
    "            )\n",
    "            # Critical: Ensure pad token and ID are set for batching\n",
    "            if tokenizer.pad_token is None or tokenizer.pad_token_id is None:\n",
    "                if tokenizer.eos_token is not None and tokenizer.eos_token_id is not None:\n",
    "                    logger.warning(f\"Tokenizer for {model_id_or_path} lacks pad token/ID. Setting to EOS token: '{tokenizer.eos_token}' (ID: {tokenizer.eos_token_id}).\")\n",
    "                    tokenizer.pad_token = tokenizer.eos_token\n",
    "                    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "                else:\n",
    "                    # This is a problem - no EOS token either! Add a default?\n",
    "                    logger.error(f\"CRITICAL: Tokenizer for {model_id_or_path} lacks BOTH pad and EOS tokens. Cannot set default padding. Batching will likely fail.\")\n",
    "                    # You might need to manually add a pad token here if this occurs:\n",
    "                    # tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "                    # model.resize_token_embeddings(len(tokenizer)) # If model is loaded separately\n",
    "                    # For pipeline, this is harder. Best to fix the tokenizer source.\n",
    "                    return None # Indicate failure\n",
    "\n",
    "            logger.info(f\"Tokenizer loaded. Pad token: '{tokenizer.pad_token}', ID: {tokenizer.pad_token_id}, Padding side: {tokenizer.padding_side}\")\n",
    "            tokenizer_cache[normalized_path] = tokenizer # Cache it using normalized path\n",
    "            return tokenizer\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load tokenizer {model_id_or_path}: {e}\", exc_info=True)\n",
    "            return None\n",
    "\n",
    "# --- Main Execution Logic ---\n",
    "def main(args: SimpleNamespace):\n",
    "    \"\"\"Orchestrates dataset loading, model evaluation, and result reporting.\"\"\"\n",
    "    overall_start_time = time.time()\n",
    "    results = {}\n",
    "\n",
    "    # Load and prepare the dataset once\n",
    "    try:\n",
    "        dataset_full = get_gsm8k_dataset(cache_dir=args.cache_dir, split=\"test\")\n",
    "    except Exception:\n",
    "        logger.error(\"Failed to load the dataset. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Subset the dataset if requested\n",
    "    total_samples = len(dataset_full)\n",
    "    eval_size = total_samples\n",
    "    if 0.0 < args.eval_fraction < 1.0:\n",
    "        eval_size = max(1, int(total_samples * args.eval_fraction)) # Ensure at least 1\n",
    "        logger.info(f\"Selecting {eval_size} samples ({args.eval_fraction*100:.1f}%) from {total_samples} using seed={SEED}...\")\n",
    "        dataset = dataset_full.shuffle(seed=SEED).select(range(eval_size))\n",
    "    elif args.eval_fraction == 1.0:\n",
    "         logger.info(f\"Evaluating on the full test set ({total_samples} samples).\")\n",
    "         dataset = dataset_full\n",
    "    else:\n",
    "        # This validation is also done at startup, but good to have defense in depth\n",
    "        logger.error(f\"Invalid eval_fraction: {args.eval_fraction}. Must be > 0.0 and <= 1.0. Exiting.\")\n",
    "        return\n",
    "\n",
    "    if len(dataset) == 0:\n",
    "        logger.error(\"Dataset is empty after potential filtering/subsetting. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # --- Evaluate Base Model ---\n",
    "    logger.info(f\"\\n--- Evaluating Base Model: {args.base_model} ---\")\n",
    "    base_tokenizer = get_tokenizer(args.base_model, args.cache_dir)\n",
    "    if base_tokenizer:\n",
    "        # Prepare prompts using the *original (potentially subsetted) dataset* and base tokenizer\n",
    "        base_dataset_prepared = prepare_prompts(dataset, base_tokenizer)\n",
    "        if len(base_dataset_prepared) > 0:\n",
    "             base_acc = evaluate_model(\n",
    "                 args.base_model, base_dataset_prepared, args.batch_size,\n",
    "                 base_tokenizer, args.debug_sample_output # Pass debug flag\n",
    "             )\n",
    "             results[\"base_model\"] = base_acc\n",
    "        else:\n",
    "             logger.error(\"Base dataset preparation resulted in 0 samples. Skipping base model eval.\")\n",
    "             results[\"base_model\"] = \"Eval Skipped (0 samples)\"\n",
    "    else:\n",
    "         logger.error(\"Skipping base model evaluation due to tokenizer load failure.\")\n",
    "         results[\"base_model\"] = \"Eval Failed (Tokenizer Load)\"\n",
    "    # ---------------------------\n",
    "\n",
    "    # --- Evaluate Checkpoints ---\n",
    "    logger.info(f\"\\n--- Evaluating Checkpoints in: {args.output_dir} ---\")\n",
    "    if not os.path.isdir(args.output_dir):\n",
    "        logger.warning(f\"Checkpoint directory not found: {args.output_dir}. Skipping checkpoint evaluation.\")\n",
    "    else:\n",
    "        checkpoints = []\n",
    "        try:\n",
    "            ckpt_dirs = [d for d in os.listdir(args.output_dir) if d.startswith(\"checkpoint-\") and os.path.isdir(os.path.join(args.output_dir, d))]\n",
    "            # Extract step number, handling potential errors, and sort\n",
    "            valid_checkpoints = []\n",
    "            for d in ckpt_dirs:\n",
    "                try:\n",
    "                    step = int(d.split(\"-\")[-1])\n",
    "                    valid_checkpoints.append((step, d))\n",
    "                except (ValueError, IndexError):\n",
    "                    logger.warning(f\"Could not parse step number from directory name: {d}\")\n",
    "            checkpoints = sorted(valid_checkpoints, key=lambda x: x[0]) # Sort by step number\n",
    "            logger.info(f\"Found {len(checkpoints)} valid checkpoint directories.\")\n",
    "        except OSError as e:\n",
    "            logger.error(f\"Cannot access checkpoint directory {args.output_dir}: {e}\", exc_info=True)\n",
    "            checkpoints = [] # Ensure checkpoints is empty list on error\n",
    "\n",
    "        if not checkpoints:\n",
    "             logger.warning(f\"No valid 'checkpoint-<number>' directories found in {args.output_dir}.\")\n",
    "\n",
    "        # Simplified Checkpoint Loop\n",
    "        for step, ckpt_name in checkpoints:\n",
    "            ckpt_path = os.path.join(args.output_dir, ckpt_name)\n",
    "            logger.info(f\"\\n--- Evaluating Checkpoint: {ckpt_name} (Step: {step}) ---\")\n",
    "\n",
    "            # Determine tokenizer path: checkpoint's own or fallback to base\n",
    "            # Check for tokenizer files within the specific checkpoint directory\n",
    "            tokenizer_config_path = os.path.join(ckpt_path, \"tokenizer_config.json\")\n",
    "            if os.path.exists(tokenizer_config_path):\n",
    "                tokenizer_path = ckpt_path # Use checkpoint's tokenizer\n",
    "                logger.info(f\"Found tokenizer config in {ckpt_path}. Using checkpoint-specific tokenizer.\")\n",
    "            else:\n",
    "                tokenizer_path = args.base_model # Fallback to base model tokenizer\n",
    "                logger.info(f\"No tokenizer config in {ckpt_path}. Using base model tokenizer ({args.base_model}).\")\n",
    "\n",
    "            # Load tokenizer (uses cache if path hasn't changed effectively)\n",
    "            ckpt_tokenizer = get_tokenizer(tokenizer_path, args.cache_dir)\n",
    "            if not ckpt_tokenizer:\n",
    "                logger.error(f\"Failed to load tokenizer from {tokenizer_path}. Skipping checkpoint {ckpt_name}.\")\n",
    "                results[ckpt_name] = \"Eval Failed (Tokenizer Load)\"\n",
    "                continue # Skip this checkpoint\n",
    "\n",
    "            # Prepare prompts using the *original (potentially subsetted) dataset* and the determined tokenizer\n",
    "            # This ensures prompts are always correctly formatted for the specific model/tokenizer being evaluated\n",
    "            logger.info(f\"Preparing prompts for {ckpt_name} using tokenizer: {tokenizer_path}\")\n",
    "            ckpt_dataset_prepared = prepare_prompts(dataset, ckpt_tokenizer)\n",
    "\n",
    "            if len(ckpt_dataset_prepared) == 0:\n",
    "                logger.error(f\"Dataset preparation for {ckpt_name} resulted in 0 samples (using tokenizer {tokenizer_path}). Skipping eval.\")\n",
    "                results[ckpt_name] = \"Eval Skipped (0 samples)\"\n",
    "                continue # Skip this checkpoint\n",
    "\n",
    "            # Evaluate the checkpoint using its specific path and the correctly prepared dataset/tokenizer\n",
    "            acc = evaluate_model(\n",
    "                ckpt_path,              # Model path is the checkpoint directory\n",
    "                ckpt_dataset_prepared,  # Dataset prepared with the right tokenizer\n",
    "                args.batch_size,\n",
    "                ckpt_tokenizer,         # The tokenizer object itself\n",
    "                args.debug_sample_output # Pass debug flag\n",
    "            )\n",
    "            results[ckpt_name] = acc\n",
    "    # --------------------------\n",
    "\n",
    "    # --- Print Final Summary ---\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" \" * 15 + \"Overall Evaluation Results Summary\")\n",
    "    print(\"=\"*60)\n",
    "    total_eval_time = time.time() - overall_start_time\n",
    "\n",
    "    # Print Base Model Result First\n",
    "    if \"base_model\" in results:\n",
    "         base_res = results.pop(\"base_model\") # Remove from dict to handle checkpoints separately\n",
    "         model_name_disp = f\"BASE: {args.base_model}\"\n",
    "         res_str = f\"{base_res:.2f}% accuracy\" if isinstance(base_res, float) else base_res\n",
    "         print(f\"{model_name_disp:<45}: {res_str}\")\n",
    "\n",
    "    # Print Checkpoint Results, sorted by step number (derived from the key)\n",
    "    sorted_ckpt_results = sorted(\n",
    "        [(k, v) for k, v in results.items() if k.startswith(\"checkpoint-\")],\n",
    "        key=lambda item: int(item[0].split('-')[-1]) # Sort by step number in the key string\n",
    "    )\n",
    "    for model_id, acc in sorted_ckpt_results:\n",
    "        res_str = f\"{acc:.2f}% accuracy\" if isinstance(acc, float) else acc\n",
    "        print(f\"{model_id:<45}: {res_str}\") # model_id is like \"checkpoint-1000\"\n",
    "\n",
    "    print(\"-\" * 60)\n",
    "    logger.info(f\"Total evaluation runtime: {total_eval_time:.2f} seconds ({total_eval_time/60:.2f} minutes).\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# --- Script Entry Point ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration Setup using SimpleNamespace and Environment Variables\n",
    "    args = SimpleNamespace()\n",
    "    args.base_model = os.environ.get(\"BASE_MODEL\", DEFAULT_BASE_MODEL)\n",
    "    args.output_dir = os.environ.get(\"OUTPUT_DIR\", DEFAULT_OUTPUT_DIR)\n",
    "    args.cache_dir = os.environ.get(\"CACHE_DIR\", DEFAULT_CACHE_DIR)\n",
    "    try:\n",
    "        args.eval_fraction = float(os.environ.get(\"EVAL_FRACTION\", DEFAULT_EVAL_FRACTION))\n",
    "    except ValueError:\n",
    "        logger.warning(f\"Invalid EVAL_FRACTION env var. Using default: {DEFAULT_EVAL_FRACTION}\")\n",
    "        args.eval_fraction = DEFAULT_EVAL_FRACTION\n",
    "    try:\n",
    "        args.batch_size = int(os.environ.get(\"BATCH_SIZE\", DEFAULT_BATCH_SIZE))\n",
    "    except ValueError:\n",
    "        logger.warning(f\"Invalid BATCH_SIZE env var. Using default: {DEFAULT_BATCH_SIZE}\")\n",
    "        args.batch_size = DEFAULT_BATCH_SIZE\n",
    "    # Read debug flag from environment, converting common \"true\" values\n",
    "    debug_env = os.environ.get(\"DEBUG_SAMPLE_OUTPUT\", str(DEFAULT_DEBUG_SAMPLE_OUTPUT)).lower()\n",
    "    args.debug_sample_output = debug_env in ['true', '1', 'yes', 'on']\n",
    "\n",
    "    # Validation\n",
    "    if not 0.0 < args.eval_fraction <= 1.0:\n",
    "         logger.error(f\"Configuration Error: eval_fraction ({args.eval_fraction}) must be > 0.0 and <= 1.0. Adjust script or EVAL_FRACTION env var.\")\n",
    "         exit(1) # Exit on invalid config\n",
    "    if args.batch_size <= 0:\n",
    "        logger.error(f\"Configuration Error: batch_size ({args.batch_size}) must be positive. Adjust script or BATCH_SIZE env var.\")\n",
    "        exit(1) # Exit on invalid config\n",
    "\n",
    "    # Log Final Configuration\n",
    "    logger.info(\"--- Final Configuration ---\")\n",
    "    logger.info(f\"Base Model         : {args.base_model}\")\n",
    "    logger.info(f\"Checkpoints Dir    : {args.output_dir}\")\n",
    "    logger.info(f\"Cache Dir          : {args.cache_dir}\")\n",
    "    logger.info(f\"Eval Fraction      : {args.eval_fraction}\")\n",
    "    logger.info(f\"Batch Size         : {args.batch_size}\")\n",
    "    logger.info(f\"Debug Sample Output: {args.debug_sample_output} (Logs 1st sample/batch if True)\")\n",
    "    logger.info(f\"Torch Dtype        : {TORCH_DTYPE}\")\n",
    "    logger.info(f\"Attention Impl.    : {ATTN_IMPLEMENTATION}\")\n",
    "    logger.info(f\"Max New Tokens     : {GENERATION_MAX_NEW_TOKENS}\")\n",
    "    logger.info(\"-------------------------\")\n",
    "\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e39356-4da6-4741-b94e-b91b3c1c76fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7130bb4-cea6-414e-8b07-4dcc27ed5ac4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format, you have to adhere to the format, only output the final answer without **ANY** additional information in the \"answer\" box.\n",
    "\n",
    "<think>\n",
    "...\n",
    "</think>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": \"what is 2+2?\"},\n",
    "]\n",
    "pipe = pipeline(\"text-generation\", model=\"Qwen/Qwen2.5-7B-Instruct-1M\", device_map='auto')\n",
    "pipe(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379e4ca8-947f-49b3-a76a-58fc085a7710",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install flash-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9698626a-3176-4b46-b85d-9c61b92e0ac1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!module load cuda/12.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8433cf19-02f8-4249-b42f-730f17d68a31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!which nvcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7407e9b-34ee-4c37-a2d6-e2dfb028cb70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2985eb-6d31-4f70-8a08-93a44f6c0d24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Flash Attention 2 not found. Install with `pip install flash-attn --no-build-isolation` for potential speedup.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.21s/it]\n",
      "WARNING:__main__:Model vocab size (152064) != Tokenizer vocab size (151665). Resizing model embeddings.\n",
      "WARNING:__main__:Model config lacks pad_token_id. Setting from tokenizer: 151643\n",
      "Generating Qwen2.5-7B-Instruct-1M:   0%|          | 0/26 [00:00<?, ?it/s]/home/henrykobs/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/henrykobs/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/henrykobs/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:650: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Generating Qwen2.5-7B-Instruct-1M: 100%|██████████| 26/26 [04:49<00:00, 11.14s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.15s/it]\n",
      "WARNING:__main__:Model vocab size (152064) != Tokenizer vocab size (151665). Resizing model embeddings.\n",
      "WARNING:__main__:Model config lacks pad_token_id. Setting from tokenizer: 151645\n",
      "Generating checkpoint-100:   0%|          | 0/26 [00:00<?, ?it/s]/home/henrykobs/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/henrykobs/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/henrykobs/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:650: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Generating checkpoint-100: 100%|██████████| 26/26 [09:35<00:00, 22.12s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.75s/it]\n",
      "WARNING:__main__:Model vocab size (152064) != Tokenizer vocab size (151665). Resizing model embeddings.\n",
      "WARNING:__main__:Model config lacks pad_token_id. Setting from tokenizer: 151645\n",
      "Generating checkpoint-200:   0%|          | 0/26 [00:00<?, ?it/s]/home/henrykobs/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/henrykobs/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/henrykobs/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:650: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Generating checkpoint-200:  69%|██████▉   | 18/26 [07:44<03:57, 29.73s/it]"
     ]
    }
   ],
   "source": [
    "# folio_evaluation_revised.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import logging  # Use logging for better feedback\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer  # No PeftModel needed here\n",
    "import re\n",
    "import glob\n",
    "import time  # Import time for potential timing if needed\n",
    "\n",
    "# --- Flash Attention Check ---\n",
    "try:\n",
    "    import flash_attn\n",
    "    _flash_attn_available = True\n",
    "    logging.info(\"Flash Attention 2 available. Will use if applicable.\")\n",
    "except ImportError:\n",
    "    _flash_attn_available = False\n",
    "    logging.warning(\"Flash Attention 2 not found. Install with `pip install flash-attn --no-build-isolation` for potential speedup.\")\n",
    "\n",
    "# --- Configuration Constants ---\n",
    "BASE_MODEL_PATH = \"Qwen/Qwen2.5-7B-Instruct-1M\"\n",
    "TRAINING_OUTPUT_DIR = \"cai6307-henrykobs/model/Qwen-7B-GRPO\"\n",
    "CACHE_DIR = \"cai6307-henrykobs/cache\"\n",
    "EVALUATION_ROOT_DIR = \"./folio_checkpoint_evaluation_revised_changed_prompt\"  # Updated output dir name\n",
    "BATCH_SIZE = 8  # Adjust based on memory\n",
    "MAX_LENGTH = 1536  # Input context window\n",
    "MAX_NEW_TOKENS = 1024  # Max output tokens\n",
    "TORCH_DTYPE = torch.bfloat16  # Consistent dtype\n",
    "ATTN_IMPLEMENTATION = \"flash_attention_2\" if _flash_attn_available else \"eager\"\n",
    "\n",
    "# --- Logging Setup ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Hugging Face Login ---\n",
    "try:\n",
    "    os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "    login(token=\"\")\n",
    "    logger.info(\"Hugging Face login successful.\")\n",
    "except Exception as e:\n",
    "    logger.warning(f\"Hugging Face login failed: {e}. Ensure HF_TOKEN is set. Public models might still work.\")\n",
    "\n",
    "# --- System Prompt (FOLIO Task) ---\n",
    "SYSTEM_PROMPT = \"\"\"Respond in the following format, you have to adhere to the format, only output the final answer without **ANY** additional information in the \"answer\" box.\n",
    "\n",
    "<think>\n",
    "...\n",
    "</think>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "# --- Helper Functions (Dataset Loading, Answer Extraction, Prompt Prep) ---\n",
    "\n",
    "def load_folio_dataset():\n",
    "    \"\"\"Load the FOLIO dataset validation split.\"\"\"\n",
    "    try:\n",
    "        logger.info(\"Loading FOLIO dataset (validation split)...\")\n",
    "        dataset = load_dataset(\"yale-nlp/FOLIO\", split=\"validation\", cache_dir=CACHE_DIR)\n",
    "        logger.info(f\"Loaded {len(dataset)} validation examples from FOLIO dataset.\")\n",
    "        return dataset\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading FOLIO dataset: {e}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "def extract_answer_with_status(response):\n",
    "    \"\"\"Parse model responses for FOLIO (True/False/Uncertain).\"\"\"\n",
    "    response = response.strip()\n",
    "    answer_pattern = re.compile(r'<answer>(.*?)</answer>', re.DOTALL | re.IGNORECASE)\n",
    "    match = answer_pattern.search(response)\n",
    "\n",
    "    if match:\n",
    "        answer = match.group(1).strip().lower()\n",
    "        if answer == \"true\":\n",
    "            return \"True\", \"TAG_SUCCESS\"\n",
    "        if answer == \"false\":\n",
    "            return \"False\", \"TAG_SUCCESS\"\n",
    "        if answer == \"uncertain\":\n",
    "            return \"Uncertain\", \"TAG_SUCCESS\"\n",
    "        words = answer.split()\n",
    "        if \"true\" in words:\n",
    "            return \"True\", \"TAG_FALLBACK\"\n",
    "        if \"false\" in words:\n",
    "            return \"False\", \"TAG_FALLBACK\"\n",
    "        if \"uncertain\" in words:\n",
    "            return \"Uncertain\", \"TAG_FALLBACK\"\n",
    "\n",
    "    response_lower = response.lower()\n",
    "    if re.search(r'[.>:\\s]\\s*(true)\\s*$', response_lower):\n",
    "        return \"True\", \"REGEX_FALLBACK\"\n",
    "    if re.search(r'[.>:\\s]\\s*(false)\\s*$', response_lower):\n",
    "        return \"False\", \"REGEX_FALLBACK\"\n",
    "    if re.search(r'[.>:\\s]\\s*(uncertain)\\s*$', response_lower):\n",
    "        return \"Uncertain\", \"REGEX_FALLBACK\"\n",
    "\n",
    "    words = response_lower.split()\n",
    "    if \"true\" in words:\n",
    "        return \"True\", \"KEYWORD_FALLBACK\"\n",
    "    if \"false\" in words:\n",
    "        return \"False\", \"KEYWORD_FALLBACK\"\n",
    "    if \"uncertain\" in words:\n",
    "        return \"Uncertain\", \"KEYWORD_FALLBACK\"\n",
    "\n",
    "    return \"Uncertain\", \"DEFAULT_UNCERTAIN\"\n",
    "\n",
    "def prepare_prompt(example, tokenizer):\n",
    "    \"\"\"Format FOLIO example into chat prompt.\"\"\"\n",
    "    premises = example[\"premises\"]\n",
    "    conclusion = example[\"conclusion\"]\n",
    "    user_message = f\"\"\"Given the following premises, determine if the conclusion is True, False, or Uncertain in the answer box.\n",
    "\n",
    "Premises:\n",
    "{premises}\n",
    "\n",
    "Conclusion:\n",
    "{conclusion}\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "    try:\n",
    "        if not tokenizer.chat_template and tokenizer.default_chat_template:\n",
    "            tokenizer.chat_template = tokenizer.default_chat_template\n",
    "            # logger.debug(\"Applied default chat template to tokenizer.\")  # Use debug if too verbose\n",
    "        elif not tokenizer.chat_template:\n",
    "            logger.warning(\"Tokenizer does not have a chat template. Prompt formatting might be incorrect.\")\n",
    "            prompt_string = SYSTEM_PROMPT + \"\\n\" + user_message + \"\\nAssistant:\"\n",
    "            return prompt_string\n",
    "\n",
    "        prompt_string = tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        return prompt_string\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error applying chat template: {e}\", exc_info=True)\n",
    "        try:\n",
    "            prompt_string = SYSTEM_PROMPT + \"\\n\" + user_message + \"\\nAssistant:\"\n",
    "            logger.warning(\"Falling back to basic prompt concatenation due to template error.\")\n",
    "            return prompt_string\n",
    "        except Exception as e_fallback:\n",
    "            logger.error(f\"Fallback prompt formatting also failed: {e_fallback}\")\n",
    "            return \"Error: Prompt formatting failed\"\n",
    "\n",
    "# --- Tokenizer Loading Helper (Adopted from Script 2) ---\n",
    "tokenizer_cache = {}\n",
    "\n",
    "def get_tokenizer(model_id_or_path: str, base_model_path_fallback: str, cache_dir: str):\n",
    "    \"\"\"Loads and caches tokenizer, checking checkpoint dir first, ensures pad token.\"\"\"\n",
    "    tokenizer_path_to_load = base_model_path_fallback  # Default to base\n",
    "    load_source = \"base model fallback\"\n",
    "\n",
    "    # Check if model_id_or_path is a directory (i.e., a checkpoint)\n",
    "    if os.path.isdir(model_id_or_path):\n",
    "        # Check if tokenizer files exist within the checkpoint directory\n",
    "        tokenizer_config_path = os.path.join(model_id_or_path, \"tokenizer_config.json\")\n",
    "        if os.path.exists(tokenizer_config_path):\n",
    "            tokenizer_path_to_load = model_id_or_path  # Use checkpoint's tokenizer path\n",
    "            load_source = \"checkpoint directory\"\n",
    "            logger.info(f\"Found tokenizer config in {model_id_or_path}. Will load tokenizer from checkpoint.\")\n",
    "        else:\n",
    "            logger.info(f\"No tokenizer config in {model_id_or_path}. Will load tokenizer from base model: {base_model_path_fallback}.\")\n",
    "\n",
    "    # Use the absolute path (if local) or ID as the cache key\n",
    "    cache_key = os.path.abspath(tokenizer_path_to_load) if os.path.exists(tokenizer_path_to_load) else tokenizer_path_to_load\n",
    "\n",
    "    if cache_key in tokenizer_cache:\n",
    "        logger.info(f\"Using cached tokenizer (originally from {load_source}): {tokenizer_path_to_load}\")\n",
    "        return tokenizer_cache[cache_key]\n",
    "    else:\n",
    "        logger.info(f\"Loading tokenizer from {load_source}: {tokenizer_path_to_load}...\")\n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\n",
    "                tokenizer_path_to_load,\n",
    "                trust_remote_code=True,\n",
    "                cache_dir=cache_dir,\n",
    "                padding_side='left'  # Set padding side during load\n",
    "            )\n",
    "            # Critical: Ensure pad token and ID are set for batching\n",
    "            if tokenizer.pad_token_id is None:\n",
    "                if tokenizer.eos_token_id is not None:\n",
    "                    logger.warning(f\"Tokenizer lacks pad_token_id. Setting to eos_token_id ({tokenizer.eos_token_id}).\")\n",
    "                    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "                    tokenizer.pad_token = tokenizer.eos_token  # Also set the token string\n",
    "                else:\n",
    "                    # Add a pad token if EOS is also missing (less common but possible)\n",
    "                    logger.error(\"CRITICAL: Tokenizer lacks BOTH pad and EOS tokens. Adding '[PAD]'. Model resizing needed.\")\n",
    "                    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "                    # The model resize will happen after model loading\n",
    "\n",
    "            logger.info(f\"Tokenizer loaded. Pad token: '{tokenizer.pad_token}', ID: {tokenizer.pad_token_id}, Padding side: {tokenizer.padding_side}\")\n",
    "            tokenizer_cache[cache_key] = tokenizer  # Cache it\n",
    "            return tokenizer\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load tokenizer {tokenizer_path_to_load}: {e}\", exc_info=True)\n",
    "            return None  # Indicate failure\n",
    "\n",
    "# --- Generation Function (REVISED loading logic) ---\n",
    "\n",
    "def generate_predictions_for_checkpoint(\n",
    "    model_path,  # Path to checkpoint dir OR base model ID\n",
    "    dataset,\n",
    "    batch_size,\n",
    "    max_length,\n",
    "    max_new_tokens,\n",
    "    checkpoint_output_dir,\n",
    "    cache_dir\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads model (base or checkpoint+adapter) automatically using from_pretrained\n",
    "    and generates predictions.\n",
    "    \"\"\"\n",
    "    model = None\n",
    "    tokenizer = None\n",
    "    all_completions = []\n",
    "\n",
    "    logger.info(f\"\\n--- Evaluating: {model_path} ---\")\n",
    "    logger.info(f\"Using dtype={TORCH_DTYPE}, attn='{ATTN_IMPLEMENTATION}'\")\n",
    "\n",
    "    try:\n",
    "        # --- Load Tokenizer (using helper function) ---\n",
    "        tokenizer = get_tokenizer(model_path, BASE_MODEL_PATH, cache_dir)\n",
    "        if not tokenizer:\n",
    "            raise ValueError(f\"Failed to load tokenizer for {model_path}. Cannot proceed.\")\n",
    "\n",
    "        # --- Load Model (Simplified - AutoModel handles adapters) ---\n",
    "        logger.info(f\"Loading model using AutoModelForCausalLM from: {model_path}\")\n",
    "        # This will load the base model + adapter if model_path is a checkpoint dir with adapter_config.json\n",
    "        # OR it will load just the base model if model_path is the base model ID.\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=TORCH_DTYPE,\n",
    "            device_map=\"auto\",  # Let HF handle device placement\n",
    "            # low_cpu_mem_usage=True,  # Generally good for large models\n",
    "            cache_dir=cache_dir,\n",
    "            trust_remote_code=True,  # Often needed for custom model code (like Qwen)\n",
    "            attn_implementation=ATTN_IMPLEMENTATION  # Use flash attention if available\n",
    "        )\n",
    "\n",
    "        # --- Resize embeddings AFTER model load ---\n",
    "        # Important if tokenizer added tokens (e.g., pad token)\n",
    "        # Compare model vocab size with tokenizer vocab size\n",
    "        if model.config.vocab_size != len(tokenizer):\n",
    "            logger.warning(f\"Model vocab size ({model.config.vocab_size}) != Tokenizer vocab size ({len(tokenizer)}). Resizing model embeddings.\")\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "            # Check if vocab size matches after resize\n",
    "            if model.config.vocab_size != len(tokenizer):\n",
    "                logger.error(\"Resizing token embeddings failed! Vocab sizes still mismatch.\")\n",
    "        else:\n",
    "            logger.info(\"Model and tokenizer vocab sizes match.\")\n",
    "\n",
    "        # --- Set Pad Token ID in Model Config ---\n",
    "        # Crucial for generation if tokenizer's pad_token_id was initially None\n",
    "        if model.config.pad_token_id is None:\n",
    "            # Check if tokenizer has a valid pad_token_id first\n",
    "            if tokenizer.pad_token_id is not None:\n",
    "                logger.warning(f\"Model config lacks pad_token_id. Setting from tokenizer: {tokenizer.pad_token_id}\")\n",
    "                model.config.pad_token_id = tokenizer.pad_token_id\n",
    "            else:\n",
    "                # This case should be handled by get_tokenizer, but double-check\n",
    "                logger.error(\"CRITICAL: Model config and tokenizer both lack pad_token_id even after tokenizer loading. Batch generation may fail.\")\n",
    "\n",
    "        model.eval()\n",
    "        # *** THIS IS THE CORRECTED LINE ***\n",
    "        logger.info(f\"Model loaded onto device(s): {model.device if hasattr(model, 'device') else 'Distributed (device_map=auto)'}\")\n",
    "\n",
    "        # --- Prepare Prompts ---\n",
    "        logger.info(\"Preparing prompts for FOLIO task...\")\n",
    "        prompts = []\n",
    "        prompt_errors = 0\n",
    "        for i, example in enumerate(dataset):\n",
    "            prompt_str = prepare_prompt(example, tokenizer)\n",
    "            if prompt_str == \"Error: Prompt formatting failed\":\n",
    "                logger.warning(f\"Skipping example {i} due to prompt formatting error.\")\n",
    "                prompts.append(None)  # Placeholder for filtering\n",
    "                prompt_errors += 1\n",
    "            else:\n",
    "                prompts.append({\"prompt\": prompt_str})\n",
    "\n",
    "        # Filter out errored prompts if any\n",
    "        valid_prompts = [p[\"prompt\"] for p in prompts if p is not None]\n",
    "        original_indices = [i for i, p in enumerate(prompts) if p is not None]  # Keep track if needed, not strictly used below currently\n",
    "        logger.info(f\"Prepared {len(valid_prompts)} valid prompts ({prompt_errors} errors).\")\n",
    "\n",
    "        if not valid_prompts:\n",
    "            raise ValueError(\"No valid prompts could be generated. Cannot proceed.\")\n",
    "\n",
    "        # --- Batch Generation ---\n",
    "        logger.info(f\"Generating completions in batches of {batch_size}...\")\n",
    "        generated_outputs = [\"Error: Generation not run\"] * len(valid_prompts)  # Initialize placeholders based on valid prompts count\n",
    "\n",
    "        progress_bar = tqdm(range(0, len(valid_prompts), batch_size), desc=f\"Generating {os.path.basename(model_path)}\")\n",
    "        for i in progress_bar:\n",
    "            batch_prompts = valid_prompts[i: i + batch_size]\n",
    "\n",
    "            try:\n",
    "                inputs = tokenizer(\n",
    "                    batch_prompts,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,  # Pad batch to the longest sequence\n",
    "                    truncation=True,\n",
    "                    max_length=max_length  # Max input length\n",
    "                ).to(model.device)  # Ensure batch is on the same device as model\n",
    "\n",
    "                # Double-check pad_token_id just before generation\n",
    "                gen_pad_token_id = model.config.pad_token_id if model.config.pad_token_id is not None else tokenizer.pad_token_id\n",
    "                if gen_pad_token_id is None:\n",
    "                    logger.error(\"Cannot determine pad_token_id for generation. Batching will likely fail.\")\n",
    "                    # Handle error appropriately, maybe skip batch\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=max_new_tokens,  # Max length for OUTPUT\n",
    "                        do_sample=False,  # Greedy for evaluation\n",
    "                        pad_token_id=gen_pad_token_id  # CRUCIAL for batch generation\n",
    "                    )\n",
    "\n",
    "                # Decode only generated tokens\n",
    "                input_length = inputs.input_ids.shape[1]\n",
    "                decoded_batch_outputs = tokenizer.batch_decode(\n",
    "                    outputs[:, input_length:],\n",
    "                    skip_special_tokens=True\n",
    "                )\n",
    "\n",
    "                # Store results for this batch into the correct slice of generated_outputs\n",
    "                for j, completion in enumerate(decoded_batch_outputs):\n",
    "                    output_index = i + j\n",
    "                    if output_index < len(generated_outputs):  # Boundary check\n",
    "                        generated_outputs[output_index] = completion.strip()\n",
    "                    else:\n",
    "                        logger.error(f\"Index out of bounds ({output_index}) when storing batch generation results. Batch size: {len(decoded_batch_outputs)}, Total valid: {len(generated_outputs)}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"\\nError during generation for batch starting at index {i}: {e}\", exc_info=True)\n",
    "                # Mark outputs in this batch as failed\n",
    "                for j in range(len(batch_prompts)):\n",
    "                    output_index = i + j\n",
    "                    if output_index < len(generated_outputs):\n",
    "                        generated_outputs[output_index] = \"Error: Generation failed in batch\"\n",
    "\n",
    "        # --- Reconstruct full completions list including errors ---\n",
    "        # This ensures the final list aligns with the original dataset length\n",
    "        final_completions_idx = 0\n",
    "        for i in range(len(prompts)):  # Iterate through original prompt list length (including None placeholders)\n",
    "            if prompts[i] is None:  # Was a formatting error\n",
    "                all_completions.append(\"Error: Prompt formatting failed\")\n",
    "            else:\n",
    "                # Check if index is valid for generated_outputs\n",
    "                if final_completions_idx < len(generated_outputs):\n",
    "                    all_completions.append(generated_outputs[final_completions_idx])\n",
    "                    final_completions_idx += 1  # Increment only when using a valid output slot\n",
    "                else:\n",
    "                    # This indicates a logic error - more prompts were valid than outputs generated/stored\n",
    "                    logger.error(f\"Mismatch during result reconstruction at original index {i}. Expected valid output but none found (valid index {final_completions_idx} >= generated {len(generated_outputs)}).\")\n",
    "                    all_completions.append(\"Error: Mismatch during result reconstruction\")\n",
    "\n",
    "        # Final length check\n",
    "        if len(all_completions) != len(dataset):\n",
    "            logger.error(f\"CRITICAL: Final completions length ({len(all_completions)}) does not match dataset length ({len(dataset)}).\")\n",
    "\n",
    "        logger.info(f\"Finished generation for {model_path}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"FATAL ERROR during setup or generation for {model_path}: {e}\", exc_info=True)\n",
    "        # Fill remaining completions if needed\n",
    "        num_expected = len(dataset)\n",
    "        num_generated = len(all_completions)\n",
    "        if num_generated < num_expected:\n",
    "            logger.error(f\"Appending {num_expected - num_generated} error messages due to failure.\")\n",
    "            all_completions.extend([\"Error: Checkpoint evaluation failed\"] * (num_expected - num_generated))\n",
    "        all_completions = all_completions[:num_expected]  # Ensure correct length\n",
    "\n",
    "    finally:\n",
    "        # --- Resource Cleanup ---\n",
    "        logger.info(f\"Cleaning up resources for {model_path}...\")\n",
    "        del model\n",
    "        del tokenizer\n",
    "        # Clearing the global tokenizer cache might cause reloads if base tokenizer is used again.\n",
    "        # Consider if tokenizer_cache needs clearing or if memory usage is acceptable.\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            logger.info(\"CUDA cache cleared.\")\n",
    "\n",
    "    return all_completions\n",
    "\n",
    "# --- Analysis and Reporting Functions (Mostly Unchanged) ---\n",
    "\n",
    "def analyze_extraction_methods(statuses):\n",
    "    \"\"\"Analyzes distribution of how answers were extracted.\"\"\"\n",
    "    status_counts = {}\n",
    "    for status in statuses:\n",
    "        status_counts[status] = status_counts.get(status, 0) + 1\n",
    "    logger.info(\"Answer Extraction Method Distribution:\")\n",
    "    total = len(statuses)\n",
    "    if total == 0:\n",
    "        logger.info(\"  No responses to analyze.\")\n",
    "    else:\n",
    "        for status, count in sorted(status_counts.items()):\n",
    "            logger.info(f\"  {status}: {count} ({count/total*100:.2f}%)\")\n",
    "\n",
    "def analyze_response_distribution(predictions):\n",
    "    \"\"\"Analyzes distribution of predicted labels (True/False/Uncertain).\"\"\"\n",
    "    prediction_counts = {}\n",
    "    for pred in predictions:\n",
    "        prediction_counts[pred] = prediction_counts.get(pred, 0) + 1\n",
    "    logger.info(\"Prediction distribution:\")\n",
    "    total = len(predictions)\n",
    "    if total == 0:\n",
    "        logger.info(\"  No predictions to analyze.\")\n",
    "    else:\n",
    "        for pred, count in sorted(prediction_counts.items()):\n",
    "            logger.info(f\"  {pred}: {count} ({count/total*100:.2f}%)\")\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, output_path):\n",
    "    \"\"\"Generate visualization of confusion matrix.\"\"\"\n",
    "    try:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes, annot_kws={\"size\": 12})\n",
    "        plt.xlabel('Predicted Label', fontsize=12)\n",
    "        plt.ylabel('True Label', fontsize=12)\n",
    "        plt.title('Confusion Matrix', fontsize=14)\n",
    "        plt.xticks(fontsize=10)\n",
    "        plt.yticks(fontsize=10)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_path, dpi=150)\n",
    "        plt.close()\n",
    "        logger.info(f\"Confusion matrix saved to {output_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error plotting confusion matrix: {e}\", exc_info=True)\n",
    "\n",
    "# --- Main Evaluation Orchestration (Simplified Call) ---\n",
    "\n",
    "def run_evaluation_for_checkpoint(\n",
    "    model_path,  # Path to checkpoint dir or base model ID\n",
    "    dataset,\n",
    "    checkpoint_output_dir,\n",
    "    cache_dir\n",
    "):\n",
    "    \"\"\"Runs the full evaluation pipeline for a single model/checkpoint.\"\"\"\n",
    "    os.makedirs(checkpoint_output_dir, exist_ok=True)\n",
    "    logger.info(f\"Output directory for this run: {checkpoint_output_dir}\")\n",
    "\n",
    "    # 1. Generate Predictions (Simplified call)\n",
    "    responses = generate_predictions_for_checkpoint(\n",
    "        model_path=model_path,\n",
    "        dataset=dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        max_length=MAX_LENGTH,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        checkpoint_output_dir=checkpoint_output_dir,\n",
    "        cache_dir=cache_dir,\n",
    "    )\n",
    "\n",
    "    # Save raw responses\n",
    "    raw_responses_path = os.path.join(checkpoint_output_dir, \"raw_responses.json\")\n",
    "    try:\n",
    "        with open(raw_responses_path, \"w\") as f:\n",
    "            json.dump(responses, f, indent=2)\n",
    "        logger.info(f\"Raw responses saved to {raw_responses_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving raw responses: {e}\", exc_info=True)\n",
    "\n",
    "    # 2. Parse Predictions and Analyze Extraction\n",
    "    predictions = []\n",
    "    extraction_statuses = []\n",
    "    generation_errors = 0\n",
    "    logger.info(\"Parsing predictions...\")\n",
    "    for response in responses:\n",
    "        if isinstance(response, str) and response.startswith(\"Error:\"):\n",
    "            pred, status = \"Uncertain\", \"GENERATION_ERROR\"\n",
    "            generation_errors += 1\n",
    "        else:\n",
    "            pred, status = extract_answer_with_status(response)\n",
    "        predictions.append(pred)\n",
    "        extraction_statuses.append(status)\n",
    "    if generation_errors > 0:\n",
    "        logger.warning(f\"Encountered {generation_errors} generation/prompt errors during processing.\")\n",
    "    analyze_extraction_methods(extraction_statuses)\n",
    "    analyze_response_distribution(predictions)\n",
    "\n",
    "    # 3. Get Ground Truth Labels\n",
    "    labels = [example[\"label\"] for example in dataset]\n",
    "\n",
    "    # Length Check\n",
    "    if len(predictions) != len(labels):\n",
    "        logger.error(f\"FATAL: Mismatch between predictions ({len(predictions)}) and labels ({len(labels)}). Evaluation metrics skipped.\")\n",
    "        results = {\n",
    "            \"error\": \"Prediction/Label length mismatch\",\n",
    "            \"model_path\": model_path,\n",
    "            \"num_predictions\": len(predictions),\n",
    "            \"num_labels\": len(labels)\n",
    "        }\n",
    "        results_path = os.path.join(checkpoint_output_dir, \"evaluation_results_ERROR.json\")\n",
    "        try:\n",
    "            with open(results_path, \"w\") as f:\n",
    "                json.dump(results, f, indent=2)\n",
    "            logger.error(f\"Error details saved to {results_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save error details: {e}\")\n",
    "        return  # Exit evaluation for this checkpoint\n",
    "\n",
    "    # 4. Calculate Metrics\n",
    "    logger.info(\"Calculating metrics...\")\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    report = classification_report(labels, predictions, labels=[\"True\", \"False\", \"Uncertain\"], output_dict=True, zero_division=0)\n",
    "    cm = confusion_matrix(labels, predictions, labels=[\"True\", \"False\", \"Uncertain\"])\n",
    "\n",
    "    # Log metrics\n",
    "    logger.info(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "    logger.info(\"Classification Report (dict):\")\n",
    "    try:\n",
    "        logger.info(json.dumps(report, indent=2))\n",
    "    except Exception:\n",
    "        logger.info(str(report))\n",
    "\n",
    "    # 5. Save Results\n",
    "    results = {\n",
    "        \"model_path\": model_path,\n",
    "        \"is_checkpoint\": model_path != BASE_MODEL_PATH,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"classification_report\": report,\n",
    "        \"confusion_matrix\": cm.tolist(),\n",
    "        \"extraction_method_summary\": {status: extraction_statuses.count(status) for status in sorted(set(extraction_statuses))},\n",
    "        \"generation_prompt_errors\": generation_errors\n",
    "    }\n",
    "    results_path = os.path.join(checkpoint_output_dir, \"evaluation_results.json\")\n",
    "    try:\n",
    "        with open(results_path, \"w\") as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        logger.info(f\"Evaluation metrics saved to {results_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving evaluation metrics: {e}\", exc_info=True)\n",
    "\n",
    "    # 6. Plot Confusion Matrix\n",
    "    cm_path = os.path.join(checkpoint_output_dir, \"confusion_matrix.png\")\n",
    "    plot_confusion_matrix(cm, classes=[\"True\", \"False\", \"Uncertain\"], output_path=cm_path)\n",
    "\n",
    "    # 7. Save Detailed Predictions (Optional but useful)\n",
    "    logger.info(\"Saving detailed predictions...\")\n",
    "    prediction_details = []\n",
    "    for i, (example, pred, label, response, status) in enumerate(zip(dataset, predictions, labels, responses, extraction_statuses)):\n",
    "        prediction_details.append({\n",
    "            \"example_id\": i,  # This is the index within the evaluated subset\n",
    "            \"premises\": example[\"premises\"],\n",
    "            \"conclusion\": example[\"conclusion\"],\n",
    "            \"true_label\": label,\n",
    "            \"predicted_label\": pred,\n",
    "            \"extraction_status\": status,\n",
    "            \"is_correct\": pred == label if status != \"GENERATION_ERROR\" and label in [\"True\", \"False\", \"Uncertain\"] else None,\n",
    "            \"model_response\": response\n",
    "        })\n",
    "    details_path = os.path.join(checkpoint_output_dir, \"prediction_details.json\")\n",
    "    try:\n",
    "        with open(details_path, \"w\") as f:\n",
    "            json.dump(prediction_details, f, indent=2)\n",
    "        logger.info(f\"Detailed predictions saved to {details_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving detailed predictions: {e}\", exc_info=True)\n",
    "\n",
    "    logger.info(f\"\\n--- Finished evaluation for: {model_path} ---\")\n",
    "\n",
    "# --- Main Execution Logic ---\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"Starting FOLIO Evaluation Script (Revised Loading)\")\n",
    "    logger.info(f\"Base Model ID: {BASE_MODEL_PATH}\")\n",
    "    logger.info(f\"Training Checkpoint Dir: {TRAINING_OUTPUT_DIR}\")\n",
    "    logger.info(f\"Cache Dir: {CACHE_DIR}\")\n",
    "    logger.info(f\"Evaluation Output Root: {EVALUATION_ROOT_DIR}\")\n",
    "    logger.info(f\"Batch Size: {BATCH_SIZE}\")\n",
    "    logger.info(f\"Torch Dtype: {TORCH_DTYPE}\")\n",
    "    logger.info(f\"Attention Impl.: {ATTN_IMPLEMENTATION}\")\n",
    "\n",
    "    os.makedirs(EVALUATION_ROOT_DIR, exist_ok=True)\n",
    "    os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "    # --- Load Dataset Once ---\n",
    "    folio_val_dataset = load_folio_dataset()\n",
    "    if folio_val_dataset is None:\n",
    "        logger.error(\"Exiting due to dataset loading failure.\")\n",
    "        exit(1)\n",
    "\n",
    "    # --- Identify Models/Checkpoints ---\n",
    "    models_to_evaluate = []  # Stores paths to load (base ID or checkpoint dir)\n",
    "\n",
    "    # 1. Add the base model\n",
    "    if BASE_MODEL_PATH:\n",
    "        logger.info(f\"Adding base model for evaluation: {BASE_MODEL_PATH}\")\n",
    "        models_to_evaluate.append(BASE_MODEL_PATH)\n",
    "    else:\n",
    "        logger.warning(\"No BASE_MODEL_PATH specified, skipping base model evaluation.\")\n",
    "\n",
    "    # 2. Find checkpoint directories\n",
    "    if os.path.isdir(TRAINING_OUTPUT_DIR):\n",
    "        checkpoint_pattern = os.path.join(TRAINING_OUTPUT_DIR, \"checkpoint-*\")\n",
    "        checkpoint_dirs = glob.glob(checkpoint_pattern)\n",
    "        valid_checkpoints = []\n",
    "        for d in checkpoint_dirs:\n",
    "            if os.path.isdir(d):\n",
    "                # Basic check for adapter config - essential for auto-loading\n",
    "                if os.path.exists(os.path.join(d, \"adapter_config.json\")):\n",
    "                    try:\n",
    "                        step = int(d.split('-')[-1])\n",
    "                        valid_checkpoints.append((step, d))\n",
    "                    except (ValueError, IndexError):\n",
    "                        logger.warning(f\"Skipping directory - cannot parse step number: {os.path.basename(d)}\")\n",
    "                else:\n",
    "                    logger.warning(f\"Skipping directory - missing 'adapter_config.json': {os.path.basename(d)}\")\n",
    "\n",
    "        # Sort by step number and add paths to list\n",
    "        valid_checkpoints.sort(key=lambda x: x[0])\n",
    "        if valid_checkpoints:\n",
    "            logger.info(f\"Found {len(valid_checkpoints)} valid checkpoints in {TRAINING_OUTPUT_DIR}:\")\n",
    "            for step, path in valid_checkpoints:\n",
    "                logger.info(f\"  - Adding checkpoint: {os.path.basename(path)} (Step: {step})\")\n",
    "                models_to_evaluate.append(path)\n",
    "        else:\n",
    "            logger.warning(f\"No valid checkpoint directories with 'adapter_config.json' found in {TRAINING_OUTPUT_DIR}.\")\n",
    "    else:\n",
    "        logger.warning(f\"Training output directory '{TRAINING_OUTPUT_DIR}' not found. Skipping checkpoint evaluation.\")\n",
    "\n",
    "    # --- Run Evaluation Loop ---\n",
    "    if not models_to_evaluate:\n",
    "        logger.error(\"No models or checkpoints found to evaluate. Exiting.\")\n",
    "        exit(1)\n",
    "\n",
    "    logger.info(f\"\\nStarting evaluation for {len(models_to_evaluate)} model(s)/checkpoint(s)...\")\n",
    "\n",
    "    for model_path in models_to_evaluate:\n",
    "        # Determine output directory name\n",
    "        if model_path == BASE_MODEL_PATH:\n",
    "            model_name = BASE_MODEL_PATH.replace(\"/\", \"__\") + \"_BASE\"\n",
    "        else:\n",
    "            model_name = os.path.basename(model_path)  # e.g., \"checkpoint-1000\"\n",
    "        checkpoint_output_dir = os.path.join(EVALUATION_ROOT_DIR, model_name)\n",
    "\n",
    "        # Run evaluation\n",
    "        run_evaluation_for_checkpoint(\n",
    "            model_path=model_path,\n",
    "            dataset=folio_val_dataset,\n",
    "            checkpoint_output_dir=checkpoint_output_dir,\n",
    "            cache_dir=CACHE_DIR,\n",
    "        )\n",
    "        logger.info(\"-\" * 70)  # Separator\n",
    "\n",
    "    logger.info(\"\\nAll evaluations complete!\")\n",
    "    logger.info(f\"Results saved in subdirectories under: {EVALUATION_ROOT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5dda307a-089b-4f32-b45c-c6a31aa69b6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch.bfloat16 (Ampere+ GPU detected)\n",
      "Flash Attention 2 not found. Using 'eager' attention.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.25s/it]\n",
      "WARNING:__main__:Model config lacks pad_token_id. Setting from tokenizer: 151645\n",
      "WARNING:__main__:Model vocab size (152064) != Tokenizer vocab size (151665). Resizing model embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  Given the following premises, determine if the conclusion is True, False, or Uncertain in the answer box.  Premises:When the Monkeypox virus occurs in a being, it may get Monkeypox. Monkeypox virus can occur in certain animals. Humans are mammals. Mammals are animals. Symptoms of Monkeypox include fever, headache, muscle pains, and tiredness. People feel tired when they get the flu.   Conclusion:No one gets the flu. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Assistant: <think>\n",
      "Premises provided:\n",
      "1. If the Monkeypox virus occurs in a being, that may get Monkeypox.\n",
      "2. Monkeypox virus can occur in certain animals.\n",
      "3. Humans are mammals.\n",
      "4. Mammals are animals.\n",
      "5. Symptoms of Monkeypox include fever, headache, muscle pains, and tiredness.\n",
      "6. People feel tired when they get the flu.\n",
      "Conclusion: No one gets the flu.\n",
      "\n",
      "To analyze this, we need to check if there's any logical connection between getting Monkeypox and not getting the flu based on the given premises. \n",
      "\n",
      "Premise 5 states that people feel tired when they get the flu, while premise  tells us that symptoms of Monkeypox include tiredness. However, tiredness alone does not necessarily imply that someone has the flu or Monkeypox; it could be due to other reasons. The premises do not establish a direct relationship or exclusivity between feeling tired and having the flu or Monkeypox.\n",
      "\n",
      "Therefore, the premises do not provide enough information to conclude that no one gets the flu. It's possible for someone to get the flu even not mentioned in the premises.\n",
      "</think>\n",
      "<answer>\n",
      "Uncertain\n",
      "</answer>\n",
      "------------------------------\n",
      "\n",
      "Exiting chat.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import os\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "# --- Configuration ---\n",
    "# <<<--- REPLACE THESE --->>>\n",
    "CHECKPOINT_PATH = \"cai6307-henrykobs/model/Qwen-7B-GRPO/checkpoint-400\" # Path to your specific checkpoint directory OR a Hub model ID\n",
    "BASE_MODEL_FALLBACK = \"Qwen/Qwen2.5-7B-Instruct-1M\" # Base model used for training (important fallback for tokenizer)\n",
    "CACHE_DIR = \"cai6307-henrykobs/cache\"          # Optional: Directory to cache downloaded models/tokenizers\n",
    "# <<<-------------------->>>\n",
    "\n",
    "# --- Advanced Configuration ---\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Use bf16 for Ampere GPUs (>=8.0), fp16 for older GPUs, or fp32 for CPU\n",
    "if torch.cuda.is_available():\n",
    "    if torch.cuda.get_device_capability()[0] >= 8:\n",
    "        TORCH_DTYPE = torch.bfloat16\n",
    "        print(\"Using torch.bfloat16 (Ampere+ GPU detected)\")\n",
    "    else:\n",
    "        TORCH_DTYPE = torch.float16\n",
    "        print(\"Using torch.float16 (Older GPU detected)\")\n",
    "else:\n",
    "    TORCH_DTYPE = torch.float32\n",
    "    print(\"Using torch.float32 (CPU detected)\")\n",
    "\n",
    "# Optional: Quantization (reduces memory usage, might affect performance slightly)\n",
    "# Set USE_QUANTIZATION = True to enable 4-bit quantization\n",
    "USE_QUANTIZATION = False # Set to True to try quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=TORCH_DTYPE, # Compute dtype should match TORCH_DTYPE\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ") if USE_QUANTIZATION else None\n",
    "\n",
    "# Optional: Flash Attention 2 (requires compatible GPU and installation)\n",
    "try:\n",
    "    import flash_attn\n",
    "    _flash_attn_available = True\n",
    "    ATTN_IMPLEMENTATION = \"flash_attention_2\"\n",
    "    print(\"Flash Attention 2 available. Will use.\")\n",
    "except ImportError:\n",
    "    _flash_attn_available = False\n",
    "    ATTN_IMPLEMENTATION = \"eager\" # Default attention mechanism\n",
    "    print(\"Flash Attention 2 not found. Using 'eager' attention.\")\n",
    "\n",
    "# Generation parameters\n",
    "MAX_NEW_TOKENS = 512 # Max tokens the model should generate in one turn\n",
    "TEMPERATURE = 0.6    # Controls randomness (higher = more random)\n",
    "TOP_P = 0.9          # Nucleus sampling probability\n",
    "DO_SAMPLE = True     # Whether to use sampling; False uses greedy decoding\n",
    "\n",
    "# --- Logging ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Hugging Face Login (Optional) ---\n",
    "# from huggingface_hub import login\n",
    "# try:\n",
    "#     login(token=\"hf_YOUR_TOKEN_HERE\") # Replace or use CLI login\n",
    "#     logger.info(\"Hugging Face login successful.\")\n",
    "# except Exception as e:\n",
    "#     logger.warning(f\"Hugging Face login failed or token not provided: {e}. Public models might still work.\")\n",
    "\n",
    "# --- Helper Function: Load Tokenizer ---\n",
    "# (tokenizer loading function remains the same as before)\n",
    "def get_chat_tokenizer(model_id_or_path, base_model_fallback, cache_dir):\n",
    "    \"\"\"Loads tokenizer, prioritizing the checkpoint dir, ensures pad token.\"\"\"\n",
    "    tokenizer_path_to_load = base_model_fallback\n",
    "    load_source = \"base model fallback\"\n",
    "\n",
    "    # Check if model_id_or_path is a directory with tokenizer files\n",
    "    if os.path.isdir(model_id_or_path):\n",
    "        if os.path.exists(os.path.join(model_id_or_path, \"tokenizer_config.json\")):\n",
    "            tokenizer_path_to_load = model_id_or_path\n",
    "            load_source = \"checkpoint directory\"\n",
    "            logger.info(f\"Found tokenizer config in {model_id_or_path}. Loading tokenizer from checkpoint.\")\n",
    "        else:\n",
    "            logger.info(f\"No tokenizer config in {model_id_or_path}. Attempting load from base model: {base_model_fallback}.\")\n",
    "    else:\n",
    "        # It's likely a Hub ID, attempt to load directly\n",
    "        tokenizer_path_to_load = model_id_or_path\n",
    "        load_source = \"provided ID/path\"\n",
    "        logger.info(f\"Attempting to load tokenizer directly from: {model_id_or_path}\")\n",
    "\n",
    "\n",
    "    logger.info(f\"Loading tokenizer from {load_source}: {tokenizer_path_to_load}...\")\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            tokenizer_path_to_load,\n",
    "            trust_remote_code=True,\n",
    "            cache_dir=cache_dir,\n",
    "            padding_side='left' # Important for generation\n",
    "        )\n",
    "        # Ensure pad token is set\n",
    "        if tokenizer.pad_token_id is None:\n",
    "            if tokenizer.eos_token_id is not None:\n",
    "                logger.warning(f\"Tokenizer lacks pad_token_id. Setting to eos_token_id ({tokenizer.eos_token_id}).\")\n",
    "                tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "            else:\n",
    "                # Add a pad token if EOS is also missing\n",
    "                logger.error(\"CRITICAL: Tokenizer lacks BOTH pad and EOS tokens. Adding '[PAD]'. This might require model resizing if not done during training.\")\n",
    "                tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "                # Note: Model resizing would need to happen after model load if this occurs\n",
    "\n",
    "        logger.info(f\"Tokenizer loaded successfully. Pad token ID: {tokenizer.pad_token_id}\")\n",
    "\n",
    "        # Check for chat template\n",
    "        if not tokenizer.chat_template and not tokenizer.default_chat_template:\n",
    "            logger.warning(\"Tokenizer does not have a chat_template or default_chat_template defined. Formatting might be incorrect.\")\n",
    "        elif not tokenizer.chat_template:\n",
    "             tokenizer.chat_template = tokenizer.default_chat_template\n",
    "             logger.info(\"Using default_chat_template for formatting.\")\n",
    "\n",
    "        return tokenizer\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load tokenizer from {tokenizer_path_to_load}: {e}\", exc_info=True)\n",
    "        if load_source != \"base model fallback\" and base_model_fallback:\n",
    "            logger.warning(f\"Falling back to loading tokenizer from base model: {base_model_fallback}\")\n",
    "            try:\n",
    "                tokenizer = AutoTokenizer.from_pretrained(\n",
    "                    base_model_fallback,\n",
    "                    trust_remote_code=True,\n",
    "                    cache_dir=cache_dir,\n",
    "                    padding_side='left'\n",
    "                )\n",
    "                # Repeat pad token check for fallback\n",
    "                if tokenizer.pad_token_id is None:\n",
    "                    if tokenizer.eos_token_id is not None:\n",
    "                        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "                        tokenizer.pad_token = tokenizer.eos_token\n",
    "                    else:\n",
    "                         tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "                logger.info(f\"Fallback tokenizer loaded. Pad token ID: {tokenizer.pad_token_id}\")\n",
    "                if not tokenizer.chat_template and tokenizer.default_chat_template:\n",
    "                     tokenizer.chat_template = tokenizer.default_chat_template\n",
    "                return tokenizer\n",
    "            except Exception as e_fallback:\n",
    "                 logger.error(f\"Fallback tokenizer loading also failed: {e_fallback}\", exc_info=True)\n",
    "                 return None\n",
    "        else:\n",
    "             return None\n",
    "\n",
    "\n",
    "# --- Main Chat Logic ---\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(f\"Starting interactive chat session.\")\n",
    "    logger.info(f\"Loading checkpoint/model from: {CHECKPOINT_PATH}\")\n",
    "    logger.info(f\"Using device: {DEVICE}\")\n",
    "    if USE_QUANTIZATION:\n",
    "        logger.info(\"Quantization enabled (4-bit).\")\n",
    "\n",
    "    # 1. Load Tokenizer\n",
    "    tokenizer = get_chat_tokenizer(CHECKPOINT_PATH, BASE_MODEL_FALLBACK, CACHE_DIR)\n",
    "    if not tokenizer:\n",
    "        logger.error(\"Could not load tokenizer. Exiting.\")\n",
    "        exit(1)\n",
    "\n",
    "    # Define stop tokens based on the tokenizer\n",
    "    stop_token_ids = []\n",
    "    if tokenizer.eos_token_id is not None:\n",
    "        stop_token_ids.append(tokenizer.eos_token_id)\n",
    "\n",
    "    # Add other common chat stop tokens if they exist and are different\n",
    "    im_end_token_id = tokenizer.convert_tokens_to_ids(\"<|im_end|>\")\n",
    "    if im_end_token_id != tokenizer.unk_token_id and im_end_token_id not in stop_token_ids:\n",
    "         stop_token_ids.append(im_end_token_id)\n",
    "         logger.info(f\"Adding '<|im_end|>' (ID: {im_end_token_id}) to stop tokens.\")\n",
    "\n",
    "    logger.info(f\"Stop token IDs for generation: {stop_token_ids}\")\n",
    "\n",
    "\n",
    "    # 2. Load Model\n",
    "    logger.info(\"Loading model...\")\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            CHECKPOINT_PATH,\n",
    "            torch_dtype=TORCH_DTYPE,\n",
    "            device_map=DEVICE if not USE_QUANTIZATION else None, # device_map not recommended with quantization directly here\n",
    "            quantization_config=quantization_config, # Apply quantization if enabled\n",
    "            attn_implementation=ATTN_IMPLEMENTATION,\n",
    "            trust_remote_code=True,\n",
    "            cache_dir=CACHE_DIR,\n",
    "        )\n",
    "        if DEVICE == \"cpu\" and USE_QUANTIZATION:\n",
    "             logger.warning(\"Quantization is primarily for GPU acceleration and memory saving. Performance on CPU might be suboptimal.\")\n",
    "        elif USE_QUANTIZATION and DEVICE ==\"cuda\":\n",
    "             logger.info(\"Model loaded with 4-bit quantization on GPU.\")\n",
    "        elif DEVICE == \"cuda\":\n",
    "            # If not using quantization and on GPU, ensure model is on the correct device\n",
    "            if not hasattr(model, 'hf_device_map'): # Check if device_map placed it\n",
    "                 model.to(DEVICE)\n",
    "                 logger.info(f\"Model loaded to {DEVICE} with dtype {TORCH_DTYPE}.\")\n",
    "        else:\n",
    "             logger.info(f\"Model loaded to {DEVICE} with dtype {TORCH_DTYPE}.\")\n",
    "\n",
    "\n",
    "        # Ensure model pad token ID is set\n",
    "        if model.config.pad_token_id is None:\n",
    "            if tokenizer.pad_token_id is not None:\n",
    "                logger.warning(f\"Model config lacks pad_token_id. Setting from tokenizer: {tokenizer.pad_token_id}\")\n",
    "                model.config.pad_token_id = tokenizer.pad_token_id\n",
    "            else:\n",
    "                 logger.error(\"Model config and tokenizer both lack pad_token_id. Generation might fail.\")\n",
    "\n",
    "\n",
    "        # Check for vocab resize necessity\n",
    "        if model.config.vocab_size != len(tokenizer):\n",
    "            logger.warning(f\"Model vocab size ({model.config.vocab_size}) != Tokenizer vocab size ({len(tokenizer)}). Resizing model embeddings.\")\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "            if model.config.pad_token_id is None and tokenizer.pad_token_id is not None:\n",
    "                 model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "\n",
    "        model.eval() # Set model to evaluation mode\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load the model: {e}\", exc_info=True)\n",
    "        exit(1)\n",
    "\n",
    "    # 3. Initialize Conversation History\n",
    "    messages = []\n",
    "    # <<< CHANGE START >>>\n",
    "    # Set the specific system prompt using a multi-line string\n",
    "    system_prompt = \"\"\"Respond in the following format, you have to adhere to the format, only output the final answer without **ANY** additional information in the \"answer\" box.\n",
    "\n",
    "<think>\n",
    "...\n",
    "</think>\n",
    "<answer>\n",
    "...\n",
    "</answer>\"\"\"\n",
    "    if system_prompt: # Only add if system_prompt is not empty\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "        # Log only the first line for brevity if the prompt is long\n",
    "        logger.info(f\"System prompt set (first line): '{system_prompt.splitlines()[0]}...'\")\n",
    "    # <<< CHANGE END >>>\n",
    "\n",
    "\n",
    "    logger.info(\"\\nModel loaded. Type your message or 'quit' to exit.\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # 4. Interaction Loop\n",
    "    while True:\n",
    "        try:\n",
    "            # a. Get user input\n",
    "            user_input = input(\"You: \")\n",
    "\n",
    "            # b. Check for quit command\n",
    "            if user_input.strip().lower() == 'quit':\n",
    "                print(\"Exiting chat.\")\n",
    "                break\n",
    "\n",
    "            # c. Add user input to history\n",
    "            messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "            # d. Apply chat template\n",
    "            try:\n",
    "                prompt_string = tokenizer.apply_chat_template(\n",
    "                    messages,\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=True # Crucial! Signals model's turn\n",
    "                )\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error applying chat template: {e}. Using basic concatenation.\")\n",
    "                # Basic fallback (might not work well with chat models)\n",
    "                prompt_string = \"\"\n",
    "                # Manually add system prompt if fallback used and it exists\n",
    "                if messages and messages[0]['role'] == 'system':\n",
    "                    prompt_string += f\"system: {messages[0]['content']}\\n\"\n",
    "                    # Add remaining messages\n",
    "                    prompt_string += \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in messages[1:]])\n",
    "                else:\n",
    "                    prompt_string += \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in messages])\n",
    "                prompt_string += \"\\nassistant:\" # Add the prompt for the assistant's turn\n",
    "\n",
    "\n",
    "            # e. Tokenize the prompt\n",
    "            inputs = tokenizer(prompt_string, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "\n",
    "            # f. Generate response\n",
    "            logger.info(\"Generating response...\")\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=MAX_NEW_TOKENS,\n",
    "                    eos_token_id=stop_token_ids,\n",
    "                    pad_token_id=tokenizer.pad_token_id, # Use tokenizer's pad_token_id\n",
    "                    do_sample=DO_SAMPLE,\n",
    "                    temperature=TEMPERATURE if DO_SAMPLE else None, # Only use temp if sampling\n",
    "                    top_p=TOP_P if DO_SAMPLE else None,           # Only use top_p if sampling\n",
    "                )\n",
    "\n",
    "            # g. Decode the generated part of the output\n",
    "            output_tokens = outputs[0, inputs.input_ids.shape[1]:]\n",
    "            model_response = tokenizer.decode(output_tokens, skip_special_tokens=True)\n",
    "\n",
    "            # h. Print the response\n",
    "            print(\"-\" * 30)\n",
    "            print(f\"Assistant: {model_response}\")\n",
    "            print(\"-\" * 30)\n",
    "\n",
    "\n",
    "            # i. Add model response to history\n",
    "            messages.append({\"role\": \"assistant\", \"content\": model_response})\n",
    "\n",
    "            # Optional: Limit history length\n",
    "            max_history = 10 # Keep last 10 turns (5 user, 5 assistant) + system prompt if used\n",
    "            if len(messages) > max_history + (1 if messages and messages[0]['role']=='system' else 0):\n",
    "                logger.debug(\"Trimming conversation history.\")\n",
    "                offset = 1 if messages and messages[0]['role']=='system' else 0\n",
    "                # Keep system prompt + last max_history items\n",
    "                messages = messages[0:offset] + messages[-(max_history):]\n",
    "\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nExiting chat.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logger.error(f\"An error occurred: {e}\", exc_info=True)\n",
    "            # break # Uncomment to exit on error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b853b4-8a3d-407c-87b3-d9c205777fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Given the following premises, determine if the conclusion is True, False, or Uncertain. Output only one of the 3 options.  Premises:When the Monkeypox virus occurs in a being, it may get Monkeypox. Monkeypox virus can occur in certain animals. Humans are mammals. Mammals are animals. Symptoms of Monkeypox include fever, headache, muscle pains, and tiredness. People feel tired when they get the flu.   Conclusion:No one gets the flu. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CAI6307_PROJECTS",
   "language": "python",
   "name": "cai6307_projects"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
